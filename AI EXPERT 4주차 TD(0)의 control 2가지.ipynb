{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë¦¼ê·¸ë¦¬ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table ê·¸ë¦¬ê¸°    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table ê·¸ë¦¬ê¸°\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_q_table_arrow(q,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        â†‘       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†  â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        â†“       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   â†‘     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   â†’     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   â†“     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   â†     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      â†‘         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      â†’         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      â†“         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ \n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) prediction (ë³µìŠµ, ì°¸ê³ ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 37885.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -12.59      |     -11.01      |     -10.12      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -10.85      |      -8.53      |      -5.80      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -9.68      |      -6.06      |       3.48      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TD(0) prediction\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ì´ˆê¸°í™” : \n",
    "# Ï€â† í‰ê°€í•  ì •ì±…\n",
    "# ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë„ë¡ ì§€ì •\n",
    "# ğ‘‰â† ì„ì˜ì˜ ìƒíƒœê°€ì¹˜ í•¨ìˆ˜\n",
    "V = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "# alpha: ìŠ¤í… ì‚¬ì´ì¦ˆ\n",
    "# gamma: ê°ë§ˆê°’, í• ì¸ìœ¨\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"start TD(0) prediction\")\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    \n",
    "    # s ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    #  ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        \n",
    "        # aâ†ìƒíƒœ ğ‘  ì—ì„œ ì •ì±… Ï€ì— ì˜í•´ ê²°ì •ëœ í–‰ë™ \n",
    "        # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ê²Œ í•¨\n",
    "        action = np.random.randint(0,4)\n",
    "        \n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìˆ˜ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€™ë¥¼ ê´€ì¸¡\n",
    "        # sâ†ğ‘ '\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        \n",
    "        # V(ğ‘ )â†V(ğ‘ )+ Î±[ğ‘Ÿ+ğ›¾ğ‘‰(ğ‘ ^)âˆ’ğ‘‰(ğ‘ )]\n",
    "        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(V,2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ë„ì°©ì§€ì ìœ¼ë¡œ ë„ì°©í• ìˆ˜ë¡ ê°’ì´ ì»¤ì§`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦, ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦  í•¨ìˆ˜ ì‘ì„±\n",
    "\n",
    "* ê° ìƒíƒœì˜ í–‰ë™ê°€ì¹˜ Q(s, a)ê°€ ì €ì¥ëœ Q-tableê³¼ ì—ì´ì „íŠ¸ ê°ì²´, ì…ì‹¤ë¡  ê°’ì„ ì…ë ¥ë°›ì•„ ê° ì •ì±…ì— ë§ê²Œ ì„ íƒí•˜ê³  ì„ íƒëœ í–‰ë™ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦  í•¨ìˆ˜ ì‘ì„±\n",
    "def e_greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    \n",
    "    # np.argmax: ìµœëŒ“ê°’ ë°˜í™˜\n",
    "    greedy_action = np.argmax(Q_table[pos[0],pos[1],:])\n",
    "    \n",
    "    # [0, 0, 0, 0]\n",
    "    pr = np.zeros(4)\n",
    "    for i in range(len(agent.action)):\n",
    "        if i == greedy_action:\n",
    "            pr[i] = 1 - epsilon + epsilon/len(agent.action)\n",
    "        else:\n",
    "            pr[i] = epsilon / len(agent.action)\n",
    "\n",
    "    return np.random.choice(range(0,len(agent.action)), p=pr)    \n",
    "\n",
    "# ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ í•¨ìˆ˜ ì‘ì„±\n",
    "def greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    \n",
    "    # ìµœì ì˜ í–‰ë™ë§Œ ì„ íƒë˜ê³ , ì´ì™¸ì˜ í–‰ë™ì€ ì ˆëŒ€ ì„ íƒë˜ì§€ ì•ŠìŒ\n",
    "    return np.argmax(Q_table[pos[0],pos[1],:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 5617.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -9.20       |     -7.30       |     -5.16       |\n",
      "| -9.71     -5.47 | -7.55     -4.38 | -5.91     -6.55 |\n",
      "|     -5.24       |     -3.63       |      0.40       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -6.74       |     -4.63       |     -4.46       |\n",
      "| -7.34     -3.18 | -5.42      3.59 | -3.82     -1.42 |\n",
      "|     -3.02       |     -1.13       |      9.70       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -5.06       |     -3.86       |      9.63       |\n",
      "| -6.74     -1.92 | -4.77      9.68 |  9.66      9.68 |\n",
      "|     -5.29       |     -1.65       |      9.69       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "SARSA :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TD(0) control : SARSA\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : SARSA\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    # ë¸íƒ€ 0 -> ë³€í™”ëŸ‰ 0\n",
    "    dleta = 0\n",
    "    \n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    \n",
    "    # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "    action = e_greedy(Q_table,agent,epsilon)\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        \n",
    "        # Q_visit_count[action, pos[0],pos[1]] +=1\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'  ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ (ì˜ˆ: Îµ-greedy)\n",
    "        next_action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(ğ‘†,ğ´)â†Q(ğ‘†,ğ´) + Î±[ğ‘…+ğ›¾ğ‘„(ğ‘†',ğ´')âˆ’ğ‘„(ğ‘†,ğ´)]\n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "\n",
    "        # sâ†s^' ; aâ†a^'\n",
    "        action = next_action\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"SARSA : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"SARSA :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ë¯¸ë¡œì—ì„œ ìµœì  í–‰ë™ì„ í•™ìŠµí•¨ (í–‰ë™ê°€ì¹˜ê°€ ê°€ì¥ ë†’ì€ ë°©í–¥)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) control : Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 6362.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      1.10       |      2.56       |      4.17       |\n",
      "|  1.10      4.56 |  3.10      6.17 |  4.55      4.17 |\n",
      "|      4.56       |      6.18       |      7.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      3.10       |      4.56       |      6.17       |\n",
      "|  2.56      6.18 |  4.56      7.98 |  6.18      5.97 |\n",
      "|      6.18       |      7.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      4.56       |      6.18       |      9.97       |\n",
      "|  4.17      7.97 |  6.17      9.97 |  9.97      9.97 |\n",
      "|      4.17       |      5.97       |      9.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Q-learning :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  TD(0) contro : Q-learning\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "np.random.seed(0)\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    \n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        \n",
    "        # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "        pos = agent.get_pos()\n",
    "        action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        \n",
    "        # â˜…SARSAì™€ ì°¨ì´ì \n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ ( ì˜ˆ : greedy)\n",
    "        next_action = greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(s,a)â†Q(s,a) + Î±[r+ğ›¾  maxa'ğ‘„(s',a')âˆ’ğ‘„(s,a)] \n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"Q-learning : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env2: SARSAì™€ Q-Learning ë¹„êµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2():\n",
    "    \n",
    "    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "    goal_position = [1,3]\n",
    "    \n",
    "    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì\n",
    "    reward_list = [[road,cliff,cliff, goal],\n",
    "                   [road,road, road,  road],\n",
    "                   [road,road, road,  road]]\n",
    "    \n",
    "    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì\n",
    "    reward_list1 = [[\"road\",\"cliff\",\"cliff\", \"goal\"],\n",
    "                    [\"road\",\"road\", \"road\", \"road\"],\n",
    "                    [\"road\",\"road\", \"road\", \"road\"]]\n",
    "    \n",
    "    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)\n",
    "        # np.asarray: np.arrayì™€ ìœ ì‚¬í•˜ì§€ë§Œ, copy í›„ì— copy ë°ì´í„° ë³€ê²½ ì‹œ asarrayëŠ” ì›ë³¸ë„ ë³€ê²½\n",
    "\n",
    "    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "            \n",
    "        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    \n",
    "        elif self.reward_list1[agent.pos[0]][agent.pos[1]] == \"cliff\" or new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "            \n",
    "        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 5004.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    -21.52       |    -29.96       |    -12.23       |      4.25       |\n",
      "|-20.62    -29.96 |-29.96    -29.96 |-11.58    -11.42 |  4.20      4.58 |\n",
      "|    -15.97       |    -29.96       |    -11.86       |      4.36       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    -20.49       |    -29.95       |    -13.15       |      4.81       |\n",
      "|-17.31    -15.55 |-14.58    -10.28 |-15.33     -2.86 | -9.25     -5.61 |\n",
      "|    -13.89       |    -11.50       |     -9.28       |     -6.70       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    -15.76       |    -14.80       |     -9.22       |     -3.51       |\n",
      "|-15.35    -12.22 |-13.52     -9.99 |-11.80     -7.72 | -9.26     -8.81 |\n",
      "|    -16.03       |    -14.35       |    -11.48       |     -9.30       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "SARSA :optimal policy\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†’         |      â†’         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†’         |      â†‘         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†‘         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ENV2 , SARSA\n",
    "\n",
    "# TD(0) control : SARSA\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment2()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : SARSA\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    \n",
    "    # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "    action = e_greedy(Q_table,agent,epsilon)\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        \n",
    "        # Q_visit_count[action, pos[0],pos[1]] +=1\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'  ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "        next_action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(ğ‘†,ğ´)â†Q(ğ‘†,ğ´) + Î±[ğ‘…+ğ›¾ğ‘„(ğ‘†',ğ´')âˆ’ğ‘„(ğ‘†,ğ´)]\n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "\n",
    "        # sâ†s^' ; aâ†a^'\n",
    "        action = next_action\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"SARSA : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"SARSA :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 5452.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-learning : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|     -1.66       |    -29.95       |    -14.44       |      6.96       |\n",
      "| -1.66    -29.95 |-29.95    -29.95 |-14.24    -14.25 |  7.35      7.49 |\n",
      "|      1.50       |    -29.95       |    -14.29       |      7.22       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|      0.34       |    -29.95       |    -15.37       |      7.64       |\n",
      "| -0.51      2.79 |  1.48      4.22 |  2.75      5.83 |  4.06      3.59 |\n",
      "|      0.32       |      1.45       |      2.69       |      3.81       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|      1.47       |      2.74       |      4.17       |      5.58       |\n",
      "| -1.73      1.42 |  0.28      2.61 |  1.22      3.80 |  1.95      1.44 |\n",
      "|     -1.70       |     -0.65       |      0.51       |      1.57       |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "Q-learning :optimal policy\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†“         |      â†‘         |      â†         |      â†’         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†‘         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |\n",
      "|      â†‘         |      â†‘         |      â†‘         |      â†‘         |\n",
      "|                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ENV2 , Q-learning\n",
    "\n",
    "#  TD(0) contro : Q-learning\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment2()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "np.random.seed(0)\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    \n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        \n",
    "        # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "        pos = agent.get_pos()\n",
    "        action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ ( ì˜ˆ : greedy)\n",
    "\n",
    "        next_action = greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(s,a)â†Q(s,a) + Î±[r+ğ›¾  maxa'ğ‘„(s',a')âˆ’ğ‘„(s,a)] \n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"Q-learning : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
