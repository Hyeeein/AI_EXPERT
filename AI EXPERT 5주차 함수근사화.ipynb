{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0185cb1",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13e10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9e2b4",
   "metadata": {},
   "source": [
    "### 그림 그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b34dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q_table[i,j,:]) == q_table[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q_table[i,j,:]) == q_table[i,j,1] and np.max(q_table[i,j,:]) == q_table[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q_table[i,j,:]) == q_table[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q_table[i,j,:]) == q_table[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q_table[i,j,:]) == q_table[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca497683",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2175a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f4f1a",
   "metadata": {},
   "source": [
    "### Environment 구현\n",
    "\n",
    "# 수정사항: 도착하는 순간 done = False -> done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010f7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 현재좌표가 목적지인지 확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "            return observation, reward, done\n",
    "        \n",
    "        \n",
    "        # 6.2 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 6.3 이동 후 좌표가 미로 밖인지 확인    \n",
    "        if new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 목적지인지 확인\n",
    "        elif self.reward_list1[new_pos[0]][new_pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6efc67",
   "metadata": {},
   "source": [
    "### 함수 근사화를 이용하여 행동가치함수 학습하기\n",
    "\n",
    "- 간단한 선형 함수를 이용하여 Q Table을 근사화한다. \n",
    "- Stochastic Gradient Descent를 이용하여 근사 함수를 학습한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1ece26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "step 0\n",
      "Q[i, j, 상 ] = 0.06×i+0.28×j+0.01\n",
      "Q[i, j, 우 ] = 0.10×i+0.10×j+0.04\n",
      "Q[i, j, 하 ] = 0.01×i-0.03×j-0.05\n",
      "Q[i, j, 좌 ] = -0.00×i+0.05×j-0.06\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.01       |      0.29       |      0.57       |\n",
      "| -0.06      0.04 | -0.01      0.14 |  0.04      0.24 |\n",
      "|     -0.05       |     -0.08       |     -0.11       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.07       |      0.35       |      0.63       |\n",
      "| -0.06      0.13 | -0.01      0.24 |  0.04      0.34 |\n",
      "|     -0.04       |     -0.07       |     -0.10       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.13       |      0.41       |      0.69       |\n",
      "| -0.07      0.23 | -0.01      0.33 |  0.04      0.43 |\n",
      "|     -0.03       |     -0.06       |     -0.09       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |        ↑       |        ↑       |\n",
      "|          →     |                 |                 |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |        ↑       |        ↑       |\n",
      "|          →     |                 |                 |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |        ↑       |        ↑       |\n",
      "|          →     |                 |                 |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 400  loss: 0.9967699655382563\n",
      "Q[i, j, 상 ] = -0.34×i-0.60×j-1.04\n",
      "Q[i, j, 우 ] = -0.20×i-0.68×j-0.81\n",
      "Q[i, j, 하 ] = -0.70×i-0.23×j-0.80\n",
      "Q[i, j, 좌 ] = -0.67×i-0.30×j-1.01\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.04       |     -1.64       |     -2.23       |\n",
      "| -1.01     -0.81 | -1.31     -1.50 | -1.61     -2.18 |\n",
      "|     -0.80       |     -1.02       |     -1.25       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.37       |     -1.97       |     -2.57       |\n",
      "| -1.69     -1.01 | -1.98     -1.70 | -2.28     -2.38 |\n",
      "|     -1.50       |     -1.72       |     -1.95       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.71       |     -2.31       |     -2.91       |\n",
      "| -2.36     -1.21 | -2.66     -1.90 | -2.96     -2.58 |\n",
      "|     -2.20       |     -2.42       |     -2.65       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 800  loss: 0.7084818382282065\n",
      "Q[i, j, 상 ] = -0.34×i-0.68×j-1.42\n",
      "Q[i, j, 우 ] = -0.13×i-0.70×j-1.21\n",
      "Q[i, j, 하 ] = -0.70×i-0.15×j-1.20\n",
      "Q[i, j, 좌 ] = -0.71×i-0.32×j-1.39\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.42       |     -2.10       |     -2.77       |\n",
      "| -1.39     -1.21 | -1.72     -1.91 | -2.04     -2.61 |\n",
      "|     -1.20       |     -1.35       |     -1.51       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.76       |     -2.44       |     -3.11       |\n",
      "| -2.11     -1.34 | -2.43     -2.04 | -2.75     -2.74 |\n",
      "|     -1.90       |     -2.06       |     -2.21       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.10       |     -2.78       |     -3.45       |\n",
      "| -2.82     -1.47 | -3.14     -2.17 | -3.46     -2.87 |\n",
      "|     -2.61       |     -2.76       |     -2.91       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 1200  loss: 0.6255621833821751\n",
      "Q[i, j, 상 ] = -0.30×i-0.61×j-1.66\n",
      "Q[i, j, 우 ] = -0.00×i-0.59×j-1.52\n",
      "Q[i, j, 하 ] = -0.59×i-0.02×j-1.50\n",
      "Q[i, j, 좌 ] = -0.62×i-0.30×j-1.65\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.66       |     -2.27       |     -2.87       |\n",
      "| -1.65     -1.52 | -1.94     -2.11 | -2.24     -2.70 |\n",
      "|     -1.50       |     -1.52       |     -1.54       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.96       |     -2.57       |     -3.17       |\n",
      "| -2.27     -1.52 | -2.57     -2.11 | -2.86     -2.70 |\n",
      "|     -2.09       |     -2.11       |     -2.12       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.26       |     -2.87       |     -3.48       |\n",
      "| -2.89     -1.52 | -3.19     -2.11 | -3.48     -2.70 |\n",
      "|     -2.68       |     -2.70       |     -2.71       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 1600  loss: 0.5386708890662744\n",
      "Q[i, j, 상 ] = -0.26×i-0.51×j-1.86\n",
      "Q[i, j, 우 ] = 0.14×i-0.48×j-1.76\n",
      "Q[i, j, 하 ] = -0.49×i+0.11×j-1.75\n",
      "Q[i, j, 좌 ] = -0.51×i-0.26×j-1.85\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.86       |     -2.37       |     -2.88       |\n",
      "| -1.85     -1.76 | -2.11     -2.24 | -2.36     -2.73 |\n",
      "|     -1.75       |     -1.64       |     -1.52       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.12       |     -2.63       |     -3.14       |\n",
      "| -2.36     -1.62 | -2.62     -2.10 | -2.87     -2.58 |\n",
      "|     -2.23       |     -2.12       |     -2.01       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.38       |     -2.89       |     -3.39       |\n",
      "| -2.87     -1.47 | -3.13     -1.96 | -3.38     -2.44 |\n",
      "|     -2.72       |     -2.61       |     -2.50       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "step 2000  loss: 0.46277460081359734\n",
      "Q[i, j, 상 ] = -0.21×i-0.40×j-2.03\n",
      "Q[i, j, 우 ] = 0.25×i-0.40×j-1.96\n",
      "Q[i, j, 하 ] = -0.39×i+0.26×j-1.93\n",
      "Q[i, j, 좌 ] = -0.41×i-0.22×j-2.03\n",
      "Q Table (approximated by linear function)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.03       |     -2.44       |     -2.84       |\n",
      "| -2.03     -1.96 | -2.24     -2.36 | -2.46     -2.75 |\n",
      "|     -1.93       |     -1.67       |     -1.41       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.25       |     -2.65       |     -3.05       |\n",
      "| -2.43     -1.70 | -2.65     -2.10 | -2.87     -2.50 |\n",
      "|     -2.32       |     -2.06       |     -1.80       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.46       |     -2.87       |     -3.27       |\n",
      "| -2.84     -1.45 | -3.05     -1.85 | -3.27     -2.24 |\n",
      "|     -2.71       |     -2.45       |     -2.19       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Error Table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.97       |      0.56       |      0.16       |\n",
      "|  0.97      0.55 |  0.50     -0.08 |  0.05      0.25 |\n",
      "|      0.60       |      1.18       |      1.21       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.49       |     -0.14       |     -0.78       |\n",
      "|  0.57      1.15 | -0.11      0.52 | -0.01      0.50 |\n",
      "|     -0.02       |      0.60       |     -2.80       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.07       |     -0.01       |      0.00       |\n",
      "|  0.16      1.21 | -0.75     -2.85 |  0.00      0.00 |\n",
      "|      0.29       |      0.55       |      0.00       |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_and_show_q_table(Q, env, agent):\n",
    "    for k in range(4):\n",
    "        print(\"Q[i, j, %c ] = %.2f×i%+.2f×j%+.2f\"%(\"상우하좌\"[k], a1[k], a2[k], a3[k]))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for k in range(len(agent.action)): \n",
    "                q_table[i, j, k] = Q(i, j, k)\n",
    "    print(\"Q Table (approximated by linear function)\")\n",
    "    show_q_table(np.round(q_table,2),env)\n",
    "    print(\"Best actions Arrow\")\n",
    "    show_q_table_arrow(q_table,env)\n",
    "\n",
    "np.random.seed(7777)\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# 3 행동의 가치를 함수 형식으로 저장\n",
    "a1 = np.random.normal(scale=0.1,size=[len(agent.action)])\n",
    "a2 = np.random.normal(scale=0.1,size=[len(agent.action)])\n",
    "a3 = np.random.normal(scale=0.1,size=[len(agent.action)])\n",
    "Q = lambda i, j, act: a1[act]*i + a2[act]*j + a3[act]\n",
    "\n",
    "total_step = 2000\n",
    "batch_size = 100\n",
    "steps_for_print = 400\n",
    "gamma = 0.9\n",
    "learning_rate = 0.01\n",
    "total_loss = 0\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"step\", 0)\n",
    "find_and_show_q_table(Q, env, agent)\n",
    "\n",
    "for attmp in range(total_step):\n",
    "    #기울기 저장\n",
    "    a1_gradient = np.array([0, 0, 0, 0], dtype=float)\n",
    "    a2_gradient = np.array([0, 0, 0, 0], dtype=float)\n",
    "    a3_gradient = np.array([0, 0, 0, 0], dtype=float)\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        i, j = np.random.randint(0, 3, [2])\n",
    "        while (i, j) == (2, 2):\n",
    "            i, j = np.random.randint(0, 3, [2])\n",
    "        agent.set_pos([i,j])\n",
    "        act = np.random.randint(len(agent.action))\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        if not done:\n",
    "            next_q = -1e6\n",
    "            # Q-Learning Style\n",
    "            for k in range(len(agent.action)):\n",
    "                next_q = max( Q(observation[0], observation[1], k), next_q)\n",
    "            target = reward + gamma * next_q\n",
    "        else:\n",
    "            target = reward\n",
    "        # 오차 구하기\n",
    "        # (Mean) Square Error, MSE\n",
    "        # MSE = 1/2 Σ(Pred - Target) ^ 2 \n",
    "        loss = 1/2 * (Q(i, j, act) - target)**2\n",
    "        total_loss += loss\n",
    "        \n",
    "        # ∂Loss /∂Q\n",
    "        Q_gradient = Q(i, j, act) - target\n",
    "        \n",
    "        # ∂Loss/∂a = (∂Loss/∂Q) × (∂Q/∂a)\n",
    "        # Q = a1 × i + a2 × j + a3\n",
    "        # (∂Q/∂a1) = i, (∂Q/∂a2) = j, (∂Q/∂a3) = 1    \n",
    "        \n",
    "        #∂Loss/∂a1 = (∂Loss/∂Q) × (∂Q/∂a1) = Q_gradient × i \n",
    "        a1_gradient[act] += Q_gradient * i\n",
    "        #∂Loss/∂a2 = (∂Loss/∂Q) × (∂Q/∂a2) = Q_gradient × j\n",
    "        a2_gradient[act] += Q_gradient * j\n",
    "        #∂Loss/∂a3 = (∂Loss/∂Q) × (∂Q/∂a1) = Q_gradient × 1\n",
    "        a3_gradient[act] += Q_gradient\n",
    "    \n",
    "    # 평균 (mean)\n",
    "    a1_gradient /= batch_size\n",
    "    a2_gradient /= batch_size\n",
    "    a3_gradient /= batch_size\n",
    "    \n",
    "    # 파라미터 갱신 (훈련)\n",
    "    a1 -= learning_rate*a1_gradient\n",
    "    a2 -= learning_rate*a2_gradient\n",
    "    a3 -= learning_rate*a3_gradient\n",
    "      \n",
    "    # 주기적으로 평균 Loss 출력\n",
    "    if (attmp+1) % steps_for_print == 0:\n",
    "        print(\"=\"*30)\n",
    "        print(\"step\", attmp+1, \" loss:\", total_loss/(steps_for_print*batch_size))\n",
    "        total_loss = 0\n",
    "        find_and_show_q_table(Q, env, agent)\n",
    "        \n",
    "#4. 학습 후 차이(편차) 계산\n",
    "devi_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        if (i, j) == (2, 2):\n",
    "            continue\n",
    "        for k in range(4):\n",
    "            agent.set_pos([i,j])\n",
    "            observation, reward, done = env.move(agent, k)\n",
    "            if not done:\n",
    "                next_q = -1e6\n",
    "                # Q-Learning Style\n",
    "                for inner_k in range(len(agent.action)):\n",
    "                    next_q = max( Q(observation[0], observation[1], inner_k), next_q)\n",
    "                target = reward + gamma * next_q\n",
    "            else:\n",
    "                target = reward\n",
    "            devi_table[i,j,k] = Q(i,j,k) - target\n",
    "            \n",
    "print()\n",
    "print(\"Error Table\")\n",
    "show_q_table(np.round(devi_table,2), env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880fee8",
   "metadata": {},
   "source": [
    "### 신경망 기초\n",
    "\n",
    "#### 이후 코드는 \"밑바닥부터 시작하는 딥러닝\"의 Github 코드를 가져오고 수정하여 작성한 코드임\n",
    "\n",
    "- 신경망의 층들은 기본적으로 연산하는 forward와 출력에 대한 입력 및 파라미터들의 편미분을 구하는 backward로 나누어져 있다. 이러한 방식을 backpropagation이라고 한다.\n",
    "- forward에서 입력된 입력들을 기억해놓았다가 backward에서 gradient를 구하는데 사용된다.\n",
    "- 결과 값에 대하여 관여한 모든 파라미터들의 Gradient 값을 빠르게 구할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265c2da",
   "metadata": {},
   "source": [
    "### 행렬곱 (Affine, 어파인) 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b530c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 이루는 신경망의 파라미터들이 어떻게 학습되는지만 이해해보도록 합시다.\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        # (D, H)\n",
    "        self.W = W\n",
    "        # (H)\n",
    "        self.b = b\n",
    "        \n",
    "        #forward에서 입력된 값을 기억하고 있다가\n",
    "        #backward에서 gradient를 구할 때 사용된다.\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    # y = x W + b\n",
    "    def forward(self, x):\n",
    "        # (다차원) 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        #x의 첫번째 차원은 batch의 차원에 해당하는 부분이다.\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # (B, H)   (B, D) (D, H)        (H) : broadcasting (B, H)\n",
    "        #   y    =   x      W       +    b\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    # ∂L/∂W = x.T (∂L/∂y)\n",
    "    # ∂L/∂b = (∂L/∂y) (broadcasting에 의한 것만 덧셈해주면 됨)\n",
    "    # ∂L/∂x = (∂L/∂y) W.T\n",
    "    def backward(self, dout):\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a29b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xW + b = \n",
      "[[ 0.59798385 -4.79014224  2.4224355 ]\n",
      " [ 2.70411312 -4.489438    1.89747805]]\n",
      "\n",
      "dx = \n",
      "[[-3.07697927  2.50687006 -0.69469937  0.80254005]\n",
      " [-3.07697927  2.50687006 -0.69469937  0.80254005]]\n",
      "x의 [0, 0]과 곱해진 W 원소들의 합\n",
      "-3.0769792700509155\n",
      "\n",
      "dw = \n",
      "[[ 3.10263293  3.10263293  3.10263293]\n",
      " [ 2.0555053   2.0555053   2.0555053 ]\n",
      " [ 1.66885811  1.66885811  1.66885811]\n",
      " [-0.25249778 -0.25249778 -0.25249778]]\n",
      "W의 [0, 0]과 곱해진 x 원소들의 합\n",
      "3.102632928972259\n",
      "\n",
      "db = \n",
      "[2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 차원 설정\n",
    "# 배치 개수\n",
    "batch_size = 2\n",
    "# 입력 차원\n",
    "D = 4\n",
    "# Affine 출력 차원\n",
    "H = 3\n",
    "\n",
    "# 입력 및 파라미터 선언\n",
    "x = np.random.normal(size=[batch_size, D])\n",
    "W = np.random.normal(size=[D, H])\n",
    "b = np.random.normal(size=[H])\n",
    "\n",
    "# 어파인 층 선언\n",
    "A_layer = Affine(W, b)\n",
    "\n",
    "# 순전파\n",
    "y = A_layer.forward(x)\n",
    "\n",
    "print(\"xW + b = \")\n",
    "print(y)\n",
    "\n",
    "# 원래대로라면, 최종 출력으로 부터 이어져 온 y의 편미분 그래디언트의 자리이다\n",
    "# 여기서는 모든 원소가 1인 행렬이라고 가정하자\n",
    "dout = np.ones_like(y)\n",
    "\n",
    "#역전파\n",
    "dx = A_layer.backward(dout)\n",
    "\n",
    "print()\n",
    "print(\"dx = \")\n",
    "print(dx)\n",
    "\n",
    "print(\"x의 [0, 0]과 곱해진 W 원소들의 합\")\n",
    "print(np.sum(W[0, :]))\n",
    "\n",
    "print()\n",
    "print(\"dw = \")\n",
    "print(A_layer.dW)\n",
    "\n",
    "print(\"W의 [0, 0]과 곱해진 x 원소들의 합\")\n",
    "print(np.sum(x[:, 0]))\n",
    "\n",
    "print()\n",
    "print(\"db = \")\n",
    "print(A_layer.db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0022c7",
   "metadata": {},
   "source": [
    "### 이와 같이, 다양한 연산, 함수들에 대하여 forward와 backward를 정의할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca6329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#활성화 함수들에 대한 순전파, 역전파\n",
    "\n",
    "\n",
    "# if x < 0, y = 0\n",
    "# otherwise, y = x\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "# y = 1/(1+exp(-x))\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    # ∂L/∂W = (∂L/∂y) * (1 - y) * y\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.error = None\n",
    "    def forward(self, pred, target):\n",
    "        self.error = pred-target\n",
    "        self.loss = 0.5 * np.mean(self.error**2)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        mean_factor = 1 / self.error.size\n",
    "        dpred = mean_factor * self.error\n",
    "        return dpred\n",
    "\n",
    "# ...등등. 자세한 것은 알아서 잘 찾아보도록 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca4f03",
   "metadata": {},
   "source": [
    "### 신경망을 이용한 간단한 함수근사화\n",
    "- 이번만 직접 작성해본 신경망을 이용하여 FrozenLake의 Q값을 근사화해보자\n",
    "- 이후에서는 Tensorflow 2.x 버전을 이용하여 신경망을 설계 및 학습할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad3b89",
   "metadata": {},
   "source": [
    "### 신경망 정의\n",
    "\n",
    "- 2층 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5ec9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = MSELoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # Mask: 어떤 부분만 Loss 계산할 것인지. 1이면 Loss 계산. 0이면 제외\n",
    "    def loss(self, x, t, mask=1):\n",
    "        y = self.predict(x)\n",
    "        if np.size(mask) != 1:\n",
    "            y = np.sum(y*mask, axis=1)\n",
    "            t = np.sum(t*mask, axis=1)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "        \n",
    "    def gradient(self, x, t, mask=1):\n",
    "        # forward\n",
    "        self.loss(x, t, mask)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        # 마스크 연산 backpropagation\n",
    "        dout = mask * dout[:, np.newaxis]\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f72789",
   "metadata": {},
   "source": [
    "### 신경망을 이용한 행동가치함수의 함수 근사화 (Deep Q Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c60e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "step 0\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.00       |     -0.03       |     -0.05       |\n",
      "|  0.00      0.00 |  0.01      0.02 |  0.01      0.04 |\n",
      "|      0.00       |      0.02       |      0.05       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.03       |      0.03       |     -0.00       |\n",
      "| -0.01     -0.02 | -0.03     -0.00 | -0.02      0.02 |\n",
      "|     -0.02       |      0.01       |      0.04       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.06       |      0.06       |      0.05       |\n",
      "| -0.02     -0.05 | -0.05     -0.02 | -0.05     -0.00 |\n",
      "|     -0.03       |     -0.01       |      0.02       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑       |                 |                 |\n",
      "|      ←  →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑       |        ↑       |                 |\n",
      "|                 |                 |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑       |        ↑       |        ↑       |\n",
      "|                 |                 |                 |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 400  loss: 0.010188087118109507\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.78       |     -2.31       |     -2.84       |\n",
      "| -1.75     -1.50 | -2.18     -1.91 | -2.60     -2.34 |\n",
      "|     -1.52       |     -1.84       |     -2.17       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.20       |     -2.75       |     -3.28       |\n",
      "| -2.36     -1.92 | -2.79     -2.31 | -3.21     -2.74 |\n",
      "|     -2.07       |     -2.37       |     -2.69       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.63       |     -3.17       |     -3.71       |\n",
      "| -2.97     -2.34 | -3.40     -2.71 | -3.82     -3.14 |\n",
      "|     -2.60       |     -2.92       |     -3.23       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 800  loss: 0.0062202254915635444\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.52       |     -2.83       |     -3.14       |\n",
      "| -2.49     -2.17 | -2.66     -2.38 | -2.86     -2.60 |\n",
      "|     -2.19       |     -2.19       |     -2.22       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.65       |     -2.98       |     -3.28       |\n",
      "| -2.81     -2.23 | -2.99     -2.42 | -3.16     -2.66 |\n",
      "|     -2.49       |     -2.47       |     -2.46       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.83       |     -3.11       |     -3.42       |\n",
      "| -3.14     -2.33 | -3.32     -2.45 | -3.49     -2.69 |\n",
      "|     -2.77       |     -2.77       |     -2.76       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |          →     |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 1200  loss: 0.004245113485358695\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -3.10       |     -3.12       |     -3.13       |\n",
      "| -3.07     -2.69 | -3.00     -2.67 | -2.94     -2.69 |\n",
      "|     -2.70       |     -2.31       |     -1.95       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.95       |     -2.98       |     -2.99       |\n",
      "| -3.07     -2.35 | -2.99     -2.33 | -2.90     -2.39 |\n",
      "|     -2.76       |     -2.33       |     -1.91       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.88       |     -2.83       |     -2.84       |\n",
      "| -3.11     -2.11 | -3.00     -1.95 | -2.91     -2.02 |\n",
      "|     -2.81       |     -2.39       |     -1.97       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 1600  loss: 0.0028781030737465483\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -3.35       |     -3.18       |     -2.99       |\n",
      "| -3.32     -2.94 | -3.15     -2.78 | -2.97     -2.68 |\n",
      "|     -3.00       |     -2.24       |     -1.52       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -3.08       |     -2.90       |     -2.69       |\n",
      "| -3.12     -2.22 | -2.92     -2.09 | -2.70     -2.08 |\n",
      "|     -2.92       |     -2.11       |     -1.30       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.90       |     -2.63       |     -2.53       |\n",
      "| -2.98     -1.65 | -2.71     -1.33 | -2.61     -1.45 |\n",
      "|     -2.82       |     -2.05       |     -1.36       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "==============================\n",
      "step 2000  loss: 0.0022072670650578464\n",
      "Q Table (approximated by Neural Network)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -3.34       |     -3.06       |     -2.76       |\n",
      "| -3.31     -2.92 | -3.11     -2.77 | -2.90     -2.70 |\n",
      "|     -2.95       |     -1.96       |     -1.02       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -3.08       |     -2.75       |     -2.41       |\n",
      "| -3.04     -1.98 | -2.77     -1.85 | -2.53     -1.91 |\n",
      "|     -2.87       |     -1.88       |     -0.79       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.90       |     -2.48       |     -2.47       |\n",
      "| -2.83     -1.22 | -2.45     -0.83 | -2.57     -1.30 |\n",
      "|     -2.80       |     -1.92       |     -1.24       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "Best actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Error Table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.34       |     -0.06       |      0.24       |\n",
      "| -0.31     -0.16 |  0.52     -0.85 | -0.14      0.30 |\n",
      "|     -0.17       |      0.71       |      0.70       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.55       |      0.02       |     -0.50       |\n",
      "| -0.04      0.69 |  0.01     -0.14 |  0.14      1.09 |\n",
      "|     -0.77       |     -0.13       |     -1.79       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.12       |      0.19       |      0.00       |\n",
      "|  0.17      0.52 | -0.35     -1.83 |  0.00      0.00 |\n",
      "|      0.20       |      1.08       |      0.00       |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def find_and_show_q_table(network, env, agent):\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1], len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for k in range(len(agent.action)): \n",
    "                q_table[i, j] = network.predict(np.array([[i,j]]))[0]\n",
    "    print(\"Q Table (approximated by Neural Network)\")\n",
    "    show_q_table(np.round(q_table,2),env)\n",
    "    print(\"Best actions Arrow\")\n",
    "    show_q_table_arrow(q_table,env)\n",
    "\n",
    "\n",
    "np.random.seed(7777)\n",
    "\n",
    "\n",
    "# 상태 (i, j)를 입력으로 받아 [상, 우, 하, 좌] 행동에 대한 Q값을 반환하는 신경망\n",
    "network = TwoLayerNet(input_size=2, hidden_size=16, output_size=4, weight_init_std = 0.1)\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "total_step = 2000\n",
    "batch_size = 100\n",
    "steps_for_print = 400\n",
    "gamma = 0.9\n",
    "learning_rate = 0.01\n",
    "total_loss = 0\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"step\", 0)\n",
    "find_and_show_q_table(network, env, agent)\n",
    "\n",
    "for attmp in range(total_step):\n",
    "    # 신경망의 입력 (state)\n",
    "    x = np.zeros([batch_size, 2])\n",
    "    # 신경망의 (우리가 원하는 바람직한) 정답\n",
    "    target_y = np.zeros([batch_size, 4])\n",
    "    # 어떤 출력을 보고자 하는 것인지 나타내는 mask.\n",
    "    # mask[batch, k] = 1 : 현재 k란 행동을 수행한 경우를 학습하고 싶다\n",
    "    mask = np.zeros([batch_size, 4])\n",
    "    \n",
    "    # (target_y를 구하기 위한) 임시값들\n",
    "    observations = np.zeros([batch_size, 2])\n",
    "    rewards = np.zeros([batch_size])\n",
    "    dones = np.zeros([batch_size])\n",
    "    \n",
    "    #batch마다 데이터셋 (x, target_y, mask) 만들기\n",
    "    for batch in range(batch_size):\n",
    "        i, j = np.random.randint(0, 3, [2])\n",
    "        while (i, j) == (2, 2):\n",
    "            i, j = np.random.randint(0, 3, [2])\n",
    "        x[batch] = (i, j)\n",
    "        agent.set_pos([i,j])\n",
    "        act = np.random.randint(len(agent.action))\n",
    "        mask[batch, act] = 1\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        observations[batch] = observation\n",
    "        rewards[batch] = reward\n",
    "        dones[batch] = done\n",
    "    next_pred = network.predict(observations)\n",
    "    next_q = np.amax(next_pred, axis=1)\n",
    "    target_y += (rewards + (1-dones) * gamma * next_q)[:, np.newaxis]\n",
    "    \n",
    "    #assert(False)\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.gradient(x, target_y, mask) \n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x, target_y, mask)\n",
    "    total_loss += loss\n",
    "\n",
    "    # 주기적으로 평균 Loss 출력\n",
    "    if (attmp+1) % steps_for_print == 0:\n",
    "        print(\"=\"*30)\n",
    "        print(\"step\", attmp+1, \" loss:\", total_loss/(steps_for_print*batch_size))\n",
    "        total_loss = 0\n",
    "        find_and_show_q_table(network, env, agent)\n",
    "        \n",
    "#4. 학습 후 차이(편차) 계산\n",
    "devi_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        if (i, j) == (2, 2):\n",
    "            continue\n",
    "        for k in range(4):\n",
    "            agent.set_pos([i,j])\n",
    "            observation, reward, done = env.move(agent, k)\n",
    "            if not done:\n",
    "                next_q = np.max(network.predict(np.array([observation])))\n",
    "                target = reward + gamma * next_q\n",
    "            else:\n",
    "                target = reward\n",
    "            devi_table[i,j,k] = network.predict(np.array([[i,j]]))[0, k] - target\n",
    "            \n",
    "print()\n",
    "print(\"Error Table\")\n",
    "show_q_table(np.round(devi_table,2), env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
