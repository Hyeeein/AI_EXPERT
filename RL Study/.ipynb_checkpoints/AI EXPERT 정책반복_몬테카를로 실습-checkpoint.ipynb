{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 평가, 정책 개선\n",
    "\n",
    "여기서 policy 변수는 각 상태에 대하여 어떠한 행동을 취할 지 저장하는 행렬로 정의하였다.\n",
    "\n",
    "0, 1, 2, 3 = 위, 오른쪽, 아래, 왼쪽\n",
    "\n",
    "예:\n",
    "\n",
    "policy[0, 0] = 1 의 뜻 : 상태 (0, 0) 에서 오른쪽으로 이동하겠다\n",
    "\n",
    "policy[1, 2] = 3 의 뜻 : 상태 (1, 2) 에서 왼쪽으로 이동하게 하겠다.\n",
    "\n",
    "수식으로 표현하자면 policy[i, j] = a 라는 뜻은\n",
    "\n",
    "$$\\pi(a|s_{ij}) = 1, \\pi(a'|s_{ij}) = 0\\text{  }(a'\\in A, a'\\neq a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V table 갱신 함수\n",
    "def policy_evalution(env, agent, v_table, policy):\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        # Δ←0\n",
    "        delta = 0\n",
    "        #  v←𝑉(𝑠)\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "        # 모든 𝑠∈𝑆에 대해 :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                # 에이전트를 지정된 좌표에 위치시킨후 가치함수를 계산\n",
    "                agent.set_pos([i,j])\n",
    "                # 현재 정책의 행동을 선택\n",
    "                action = policy[i,j]\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n",
    "        # ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "        # 계산전과 계산후의 가치의 차이를 계산\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "                \n",
    "        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "        if delta < 0.000001:\n",
    "            break\n",
    "    return v_table, delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy 갱신 함수\n",
    "def policy_improvement(env, agent, v_table, policy):\n",
    "\n",
    "    gamma = 0.9  \n",
    "    \n",
    "    # policyStable ← true \n",
    "    policyStable = True\n",
    "\n",
    "    # 모든 s∈S에 대해：\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):            \n",
    "            # 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛←π(s) \n",
    "            old_action = policy[i,j]            \n",
    "            # 가능한 행동중 최댓값을 가지는 행동을 선택\n",
    "            temp_action = 0\n",
    "            temp_value =  -1e+10           \n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    temp_action = action\n",
    "                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n",
    "            # 만약 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛\"≠π(s)\"라면， \"policyStable ← False\" \n",
    "            # old-action과 새로운 action이 다른지 체크\n",
    "            if old_action != temp_action :\n",
    "                policyStable = False\n",
    "            policy[i,j] = temp_action\n",
    "    return policy, policyStable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 반복 (정책 평가 <=> 정책 개선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.54      |       0.42      |       0.65      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.44      |       0.89      |       0.96      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial random Policy π0(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ←         |      ←         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      →         |      →         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "start policy iteration\n",
      "\n",
      "Vπ0(S) delta = 0.0000009713\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -28.00      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -30.00      |     -28.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -28.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π1(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Vπ1(S) delta = 0.0000002328\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       3.12      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π2(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Vπ2(S) delta = 0.0000001885\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       6.20      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π3(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "total_time = 0.017938852310180664\n"
     ]
    }
   ],
   "source": [
    "# 정책 반복\n",
    "# 환경과 에이전트에 대한 초기 설정\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 1. 초기화\n",
    "# 모든 𝑠∈𝑆에 대해 𝑉(𝑠)∈𝑅과 π(𝑠)∈𝐴(𝑠)를 임의로 설정\n",
    "\n",
    "#shape : [h, w]\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "\n",
    "#shape : [h, w]\n",
    "#값 : 해당 상태에서 어떠한 행동을 취할 것인지 나타내는 정수\n",
    "policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"Initial random V(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "print(\"Initial random Policy π0(S)\")\n",
    "show_policy(policy,env)\n",
    "print(\"start policy iteration\")\n",
    "\n",
    "# 시작 시간을 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "max_iter_number = 20000\n",
    "for iter_number in range(max_iter_number):\n",
    "    \n",
    "    # 2.정책평가\n",
    "    v_table, delta = policy_evalution(env, agent, v_table, policy)\n",
    "\n",
    "    # 정책 평가 후 결과 표시                                            \n",
    "    print(\"\")\n",
    "    print(\"Vπ{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "    # 3.정책개선\n",
    "    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n",
    "\n",
    "    # policy 변화 저장\n",
    "    print(\"policy π{}(S)\".format(iter_number+1))\n",
    "    show_policy(policy,env)\n",
    "    # 하나라도 old-action과 새로운 action이 다르다면 '2. 정책평가'를 반복\n",
    "    if(policyStable == True):\n",
    "        break\n",
    "\n",
    "        \n",
    "print(\"total_time = {}\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에피소드 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, *args, **kwargs):\n",
    "    gamma = 0.9\n",
    "    # 에피소드를 저장할 리스트\n",
    "    episode = []\n",
    "    # 이전에 방문여부 체크\n",
    "    \n",
    "    \n",
    "    # 에이전트가 모든 상태에서 출발할 수 있게 출발지점을 무작위로 설정\n",
    "    i = np.random.randint(0,env.reward.shape[0])\n",
    "    j = np.random.randint(0,env.reward.shape[1])\n",
    "    agent.set_pos([i,j])    \n",
    "    \n",
    "    #에피소드의 수익을 초기화\n",
    "    G = 0\n",
    "    \n",
    "    #감쇄율의 지수\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    \n",
    "    # 에피소드 생성\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()            \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)\n",
    "        #[position, action, reward, dummy_G_value = 0]\n",
    "        episode.append([pos, action, reward, 0])\n",
    "\n",
    "        # 에피소드가 종료했다면 루프에서 탈출\n",
    "        if done == True:                \n",
    "            break\n",
    "            \n",
    "    # episode 순간마다 G값 구하기\n",
    "    for ep_i in range(len(episode)-1, -1, -1):\n",
    "        G = G*gamma + episode[ep_i][2] # + reward\n",
    "        episode[ep_i][3] = G\n",
    "            \n",
    "    return i, j, G, episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===episode 1===\n",
      "[0, 1]  a: up reward: -3\n",
      "total reward: -3\n",
      "G(감가율 적용한 가치):  -3.0\n",
      "===episode 2===\n",
      "[1, 1]  a: down reward: -1\n",
      "[2 1]  a: up reward: -1\n",
      "[1 1]  a: left reward: -1\n",
      "[1 0]  a: down reward: -1\n",
      "[2 0]  a: up reward: -1\n",
      "[1 0]  a: up reward: -1\n",
      "[0 0]  a: up reward: -3\n",
      "total reward: -9\n",
      "G(감가율 적용한 가치):  -6.2799130000000005\n",
      "===episode 3===\n",
      "[2, 1]  a: down reward: -3\n",
      "total reward: -3\n",
      "G(감가율 적용한 가치):  -3.0\n",
      "===episode 4===\n",
      "[2, 0]  a: right reward: -1\n",
      "[2 1]  a: right reward: 1\n",
      "[2 2]  a: right reward: 1\n",
      "total reward: 1\n",
      "G(감가율 적용한 가치):  0.71\n",
      "===episode 5===\n",
      "[1, 0]  a: right reward: -1\n",
      "[1 1]  a: up reward: -1\n",
      "[0 1]  a: left reward: -1\n",
      "[0 0]  a: up reward: -3\n",
      "total reward: -6\n",
      "G(감가율 적용한 가치):  -4.897\n",
      "===episode 6===\n",
      "[1, 2]  a: left reward: -1\n",
      "[1 1]  a: left reward: -1\n",
      "[1 0]  a: up reward: -1\n",
      "[0 0]  a: down reward: -1\n",
      "[1 0]  a: left reward: -3\n",
      "total reward: -7\n",
      "G(감가율 적용한 가치):  -5.4073\n",
      "===episode 7===\n",
      "[0, 1]  a: left reward: -1\n",
      "[0 0]  a: right reward: -1\n",
      "[0 1]  a: left reward: -1\n",
      "[0 0]  a: left reward: -3\n",
      "total reward: -6\n",
      "G(감가율 적용한 가치):  -4.897\n",
      "===episode 8===\n",
      "[2, 0]  a: right reward: -1\n",
      "[2 1]  a: right reward: 1\n",
      "[2 2]  a: right reward: 1\n",
      "total reward: 1\n",
      "G(감가율 적용한 가치):  0.71\n",
      "===episode 9===\n",
      "[0, 2]  a: up reward: -3\n",
      "total reward: -3\n",
      "G(감가율 적용한 가치):  -3.0\n",
      "===episode 10===\n",
      "[2, 2]  a: left reward: 1\n",
      "total reward: 1\n",
      "G(감가율 적용한 가치):  1.0\n"
     ]
    }
   ],
   "source": [
    "#에피소드 생성 실험\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "  print(\"===episode %d===\" % (i+1))\n",
    "  _, _, G, episode = generate_episode(env, agent, True)\n",
    "  total_reward = 0\n",
    "  for where, action, reward, G_s in episode:\n",
    "    print(where,\"\", \"a:\", [\"up\", \"right\", \"down\", \"left\"][action], \"reward:\", reward)\n",
    "    total_reward += reward\n",
    "  print(\"total reward:\", total_reward)\n",
    "  print(\"G(감가율 적용한 가치): \", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit and Every-Visit MC Prediction\n",
    "\n",
    "#### -- \"프로그래머를 위한 강화학습\" 에는 없는 내용 --\n",
    "\n",
    "First-visit MC는 (에피소드 내) 상태 s의 첫 방문시의 반환값만 고려하는 방법\n",
    "\n",
    "Every-visit MC는 (에피소드 내) 상태 s의 모든 방문의 반환값을 고려하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start first visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:03<00:00, 25901.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.80      |      -3.99      |      -3.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.01      |      -3.89      |      -2.42      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.42      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_start_count(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11302.00      |   11148.00      |   11021.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11077.00      |   11027.00      |   11109.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11151.00      |   11075.00      |   11090.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_success_pr(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.04      |       0.09      |       0.10      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.09      |       0.20      |       0.33      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.10      |       0.33      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "start every visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:04<00:00, 24727.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.81      |      -4.01      |      -3.45      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.02      |      -3.91      |      -2.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.44      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_start_count(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22405.00      |   22257.00      |   22302.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22296.00      |   22070.00      |   22076.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22137.00      |   22175.00      |   22282.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_success_pr(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.04      |       0.09      |       0.10      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.09      |       0.21      |       0.33      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.10      |       0.33      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# first-visit MC and every-visit MC prediction\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 에피소드 출발횟수를 저장하는 테이블\n",
    "v_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 도착지점 도착횟수를 저장하는 테이블\n",
    "v_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해)\n",
    "Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수를 지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit 를 사용할지 every visit를 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "\n",
    "for first_visit in [True, False]:\n",
    "    if first_visit:\n",
    "        print(\"start first visit MC\")\n",
    "    else : \n",
    "        print(\"start every visit MC\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    for epi in tqdm(range(max_episode)):\n",
    "\n",
    "        i,j,G,episode = generate_episode(env, agent)\n",
    "\n",
    "        visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "        v_start[i,j] += 1\n",
    "        for where, action, reward, G_s in episode:\n",
    "            s_i, s_j = where\n",
    "            if first_visit and visit[s_i][s_j] == 1:\n",
    "                continue\n",
    "            visit[s_i][s_j] = 1\n",
    "            Return_s[s_i][s_j].append(G_s)\n",
    "\n",
    "\n",
    "        ## 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n",
    "        #Return_s[i][j].append(G)\n",
    "        #\n",
    "        ## 에피소드 발생 횟수 계산\n",
    "        #episode_count = len(Return_s[i][j])\n",
    "        ## 상태별 발생한 수익의 총합 계산\n",
    "        #total_G = np.sum(Return_s[i][j])\n",
    "        ## 상태별 발생한 수익의 평균 계산\n",
    "        #v_table[i,j] = total_G / episode_count\n",
    "\n",
    "        # 도착지점에 도착(reward = 1)했는지 체크    \n",
    "        # episode[-1][2] : 에피소드 마지막 상태의 보상\n",
    "        if episode[-1][2] == 1:\n",
    "            v_success[i,j] += 1\n",
    "\n",
    "\n",
    "    # 에피소드 출발 횟수 저장 \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            visit_count = len(Return_s[i][j])\n",
    "            total_G = np.sum(Return_s[i][j])\n",
    "            v_table[i,j] = total_G / visit_count\n",
    "\n",
    "    print(\"V(s)\")\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print(\"V_start_count(s)\")\n",
    "    show_v_table(np.round(v_start,2),env)\n",
    "    print(\"V_success_pr(s)\")\n",
    "    show_v_table(np.round(v_success/v_start,2),env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental mean (증분 평균) 을 이용하는 몬테카를로 Prediction 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start first visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:05<00:00, 17666.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.80      |      -3.99      |      -3.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.01      |      -3.89      |      -2.42      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.42      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "start every visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:06<00:00, 15752.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.81      |      -4.01      |      -3.45      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.02      |      -3.91      |      -2.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.44      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 추가\n",
    "# 상태를 방문한 횟수를 저장하는 테이블\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 삭제\n",
    "# # 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해) : \n",
    "# Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수와 에피소드 최대 길이지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit을 사용할지 every visit을 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "for first_visit in [True, False]:\n",
    "    if first_visit:\n",
    "        print(\"start first visit MC\")\n",
    "    else : \n",
    "        print(\"start every visit MC\")\n",
    "    print()\n",
    "\n",
    "    for epi in tqdm(range(max_episode)):\n",
    "\n",
    "        i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "\n",
    "        visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "        v_start[i,j] += 1\n",
    "        for where, action, reward, G_s in episode:\n",
    "            s_i, s_j = where\n",
    "            if first_visit and visit[s_i][s_j] == 1:\n",
    "                continue\n",
    "            visit[s_i][s_j] = 1\n",
    "\n",
    "            v_visit[s_i,s_j] += 1\n",
    "            v_table[s_i,s_j] += 1 / v_visit[s_i,s_j] * (G_s - v_table[s_i,s_j])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"V(s)\")\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 코드를 실행해 보고 출력 결과를 얻어보자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
