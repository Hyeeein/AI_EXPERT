{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë¦¼ê·¸ë¦¬ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table ê·¸ë¦¬ê¸°    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table ê·¸ë¦¬ê¸°\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_q_table_arrow(q,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        â†‘       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†  â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        â†“       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   â†‘     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   â†’     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   â†“     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   â†     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      â†‘         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      â†’         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      â†“         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ \n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïµ-greedy í™•ë¥  ë³€í™”\n",
    "ì…ì‹¤ë¡ -ê·¸ë¦¬ë”” ë°©ë²• (ì…ì‹¤ë¡ -íƒìš•ì •ì±…)\n",
    "\n",
    "í•™ìŠµ ê³¼ì •ì—ì„œ Ïµ ë§Œí¼ì˜ í™•ë¥ ë¡œëŠ” ë¬´ì‘ìœ„ í–‰ë™ì„ ì·¨í•˜ê³ , 1-Ïµ ë§Œí¼ì˜ í™•ë¥ ë¡œëŠ” ê·¸ë¦¬ë””í•˜ê²Œ í–‰ë™ì„ ì„ íƒí•¨\n",
    "ë‹¤ìŒì€ í–‰ë™ a4ê°€ ìµœì ì¼ ë•Œ ì…ì‹¤ë¡ ì— ë”°ë¥¸ ê° í–‰ë™ì˜ í™•ë¥  ë³€í™”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.array([0.0, 0.0, 0.0, 1.0])\n",
    "print(\"episilon |   a1  |    a2   |    a3  |   a4  \")\n",
    "for epsilon in reversed(range(0, 11)):\n",
    "    epsilon /= 10\n",
    "    up = epsilon / len(policy)\n",
    "    right = epsilon / len(policy)\n",
    "    down = epsilon / len(policy)\n",
    "    # a4ê°€ ìµœì ì˜ ì •ì±…ì´ë¼ê³  ìƒê°í•¨\n",
    "    left = (1 -  epsilon) + (epsilon / len(policy))\n",
    "    print(\"{0:}      | {1:.3f} |  {2:.3f}  |  {3:.3f} |  {4:.3f}\".format(np.round(epsilon,3),np.round(up,3),np.round(right,3),np.round(down,3),np.round(left,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïµ-greedy ë¥¼ ì´ìš©í•œ ëª¬í…Œì¹´ë¥¼ë¡œ Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_with_policy(env, agent, policy):\n",
    "    #â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…\n",
    "    # ì¬ë°ŒëŠ”ì : gammaë¥¼ í‰ì†Œì²˜ëŸ¼ í¬ê²Œ ì¡ìœ¼ë©´ ì´ìƒí•œ ì¼ì´...?\n",
    "    #â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…\n",
    "    gamma = 0.75\n",
    "    # ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    episode = []\n",
    "    # ì´ì „ì— ë°©ë¬¸ì—¬ë¶€ ì²´í¬\n",
    "    \n",
    "    # ì—ì´ì „íŠ¸ëŠ” í•­ìƒ (0,0)ì—ì„œ ì¶œë°œ\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])    \n",
    "    #ì—í”¼ì†Œë“œì˜ ìˆ˜ìµì„ ì´ˆê¸°í™”\n",
    "    #ê°ì‡„ìœ¨ì˜ ì§€ìˆ˜\n",
    "    max_step = 100\n",
    "    # ì—í”¼ì†Œë“œ ìƒì„±\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()        \n",
    "        # í˜„ì¬ ìƒíƒœì˜ ì •ì±…ì„ ì´ìš©í•´ í–‰ë™ì„ ì„ íƒí•œ í›„ ì´ë™\n",
    "        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ, 0ê°’ (Gê°€ ì±„ì›Œì§ˆ ì˜ˆì •) ) \n",
    "        episode.append([pos,action,reward, 0])     \n",
    "\n",
    "        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œí–ˆë‹¤ë©´ ë£¨í”„ì—ì„œ íƒˆì¶œ\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    G = 0\n",
    "    # episode ìˆœê°„ë§ˆë‹¤ Gê°’ êµ¬í•˜ê¸°\n",
    "    for ep_i in range(len(episode)-1, -1, -1):\n",
    "        G = G*gamma + episode[ep_i][2] # + reward\n",
    "        episode[ep_i][3] = G\n",
    "    \n",
    "    return i, j, G, episode\n",
    "\n",
    "# first visit ë¥¼ ì‚¬ìš©í• ì§€ every visitë¥¼ ì‚¬ìš©í•  ì§€ ê²°ì •\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "for first_visit in [True, False]:\n",
    "    if first_visit:\n",
    "        print(\"start first visit MC\")\n",
    "    else : \n",
    "        print(\"start every visit MC\")\n",
    "    print()\n",
    "\n",
    "    # Ïµ-ì •ì±…ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ control ì•Œê³ ë¦¬ì¦˜\n",
    "    np.random.seed(0)\n",
    "    # í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "\n",
    "    # ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "    # # ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’ (í–‰ë™ ê°œìˆ˜, ë¯¸ë¡œ ì„¸ë¡œ, ë¯¸ë¡œ ê°€ë¡œ)\n",
    "    Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "    print(\"Initial Q(s,a)\")\n",
    "    show_q_table(Q_table,env)\n",
    "\n",
    "    # ìƒíƒœë¥¼ ë°©ë¬¸í•œ íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "    Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "    # ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "    # ê° ìƒíƒœì—ì„œ Q ê°’ì´ ê°€ì¥ í° í–‰ë™ì„ ì„ íƒ í›„ optimal_a ì— ì €ì¥\n",
    "    optimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            optimal_a[i,j] = np.argmax(Q_table[i,j,:])\n",
    "    print(\"initial optimal_a\")\n",
    "    show_policy(optimal_a,env)\n",
    "\n",
    "    # Ï€(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ğœ–âˆ’íƒìš• ì •ì±…\n",
    "    # ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ì„ íƒí•˜ë„ë¡ ì§€ì •\n",
    "    policy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "    # í•œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ê³„ì‚°\n",
    "    epsilon = 0.9\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for k in range(len(agent.action)):\n",
    "                if optimal_a[i,j] == k:\n",
    "                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "                else:\n",
    "                    policy[i,j,k] = epsilon/len(agent.action)\n",
    "    print(\"Initial Policy\")\n",
    "    show_q_table(policy,env)\n",
    "\n",
    "\n",
    "    # ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ë¥¼ ì§€ì •\n",
    "    max_episode = 10000\n",
    "\n",
    "\n",
    "\n",
    "    for epi in tqdm(range(max_episode)):\n",
    "    # for epi in range(max_episode):\n",
    "\n",
    "        # Ï€ë¥¼ ì´ìš©í•´ì„œ ì—í”¼ì†Œë“œ 1ê°œë¥¼ ìƒì„±\n",
    "        x,y,G,episode = generate_episode_with_policy(env, agent, policy)\n",
    "        visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "        for step_num in range(len(episode)):\n",
    "            # episode[step_num][0][0] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ x ì¢Œí‘œ\n",
    "            # episode[step_num][0][1] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ y ì¢Œí‘œ\n",
    "            # episode[step_num][1] : step_numë²ˆì§¸ ìƒíƒœì—ì„œ ì„ íƒí•œ í–‰ë™\n",
    "            # episode[step_num][3] : step_numë²ˆì§¸ ìƒíƒœì—ì„œ ì–»ì€ ë°˜í™˜ê°’\n",
    "            i = episode[step_num][0][0]\n",
    "            j = episode[step_num][0][1]\n",
    "            action = episode[step_num][1]\n",
    "            G = episode[step_num][3]\n",
    "            \n",
    "            if first_visit and visit[i][j] == 1:\n",
    "                continue\n",
    "            visit[i][j] = 1\n",
    "            # ì—í”¼ì†Œë“œ ì‹œì‘ì ì„ ì¹´ìš´íŠ¸\n",
    "            Q_visit[i,j,action] += 1\n",
    "            \n",
    "            # Incremental mean : ğ‘„(ğ‘ ,ğ‘)â†ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ ,ğ‘)) \n",
    "            Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])\n",
    "\n",
    "        # (c) ì—í”¼ì†Œë“œ ì•ˆì˜ ê° sì— ëŒ€í•´ì„œ :\n",
    "        # ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•  ê³µê°„ ë§ˆë ¨\n",
    "        # ğ‘âˆ— â†argmax_a ğ‘„(ğ‘ ,ğ‘)\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                optimal_a[i,j] = np.argmax(Q_table[i,j,:])            \n",
    "\n",
    "        # ëª¨ë“  ğ‘âˆˆğ´(ğ‘†) ì— ëŒ€í•´ì„œ :\n",
    "        # ìƒˆë¡œ ê³„ì‚°ëœ optimal_a ë¥¼ ì´ìš©í•´ì„œ í–‰ë™ ì„ íƒ í™•ë¥  policy (Ï€) ê°±ì‹ \n",
    "        epsilon = 0.9*(1 - epi/max_episode)\n",
    "\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                for k in range(len(agent.action)):\n",
    "                    if optimal_a[i,j] == k:\n",
    "                        policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "                    else:\n",
    "                        policy[i,j,k] = epsilon/len(agent.action)\n",
    "\n",
    "    print(\"Final Q(s,a)\")\n",
    "    show_q_table(Q_table,env)\n",
    "    print(\"Final policy\")\n",
    "    show_q_table(policy,env)\n",
    "    print(\"Final optimal_a\")\n",
    "    show_policy(optimal_a,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) prediction\n",
    "\n",
    "TDë¡œ V(s)ë¥¼ êµ¬í•´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(0) prediction\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#ì´ˆê¸°í™” : \n",
    "#Ï€â† í‰ê°€í•  ì •ì±…\n",
    "# ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë„ë¡ ì§€ì •\n",
    "#ğ‘‰â† ì„ì˜ì˜ ìƒíƒœê°€ì¹˜ í•¨ìˆ˜\n",
    "V = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"start TD(0) prediction\")\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    # s ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    #  ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # aâ†ìƒíƒœ ğ‘  ì—ì„œ ì •ì±… Ï€ì— ì˜í•´ ê²°ì •ëœ í–‰ë™ \n",
    "        # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ê²Œ í•¨\n",
    "        action = np.random.randint(0,4)\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìˆ˜ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€™ë¥¼ ê´€ì¸¡\n",
    "        # sâ†ğ‘ '\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        # V(ğ‘ )â†V(ğ‘ )+ Î±[ğ‘Ÿ+ğ›¾ğ‘‰(ğ‘ ^)âˆ’ğ‘‰(ğ‘ )]\n",
    "        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(V,2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë“  ì½”ë“œë¥¼ ì‹¤í–‰í•´ ë³´ê³  ì¶œë ¥ ê²°ê³¼ë¥¼ ì–»ì–´ë³´ì."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
