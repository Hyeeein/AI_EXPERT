{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AI EXPERT 1장 과제\n",
    "\n",
    "원본출처: https://github.com/wikibook/rlnn\n",
    "\n",
    "## 아래 코드를 실행해 보면서, 실행 결과와 각각에 대해서 적혀있는 질문의 대답에 대해서 PPT를 만들어 보세요.\n",
    ".\n",
    "## 질문은 언제나 카톡 으로 환영합니다.\n",
    "\n",
    "\n",
    "### 다음과 같은 패키지를 설치해야 합니다\n",
    "- numpy\n",
    "- matplotlib\n",
    "\n",
    "### conda를 사용한다면 설치방법\n",
    "- conda install numpy matplotlib\n",
    "\n",
    "### pip 을 사용한다면 설치방법\n",
    "- pip install numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    q = q_table\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [1,1]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road],\n",
    "                   [road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\"],\n",
    "                    [\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "            \n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif self.reward_list1[agent.pos[0]][agent.pos[1]] == \"cliff\" or\\\n",
    "            new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "            \n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 상태 가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 가치 계산\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. 현재 위치가 도착지점인지 확인\n",
    "    # 다음 상태가 도착지점이라면 도착지점의 보상 1을 반환하고, 더는 계산하지 않도록 탈출\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "    # 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 가능한 모든 행동의 보상을 계산 => 더는 계산하지 않도록 재귀함수 탈출\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n",
    "    else:\n",
    "        \n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            \n",
    "            # 4.2.1 현재 상태에서 보상을 계산\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 다음 step을 계산\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "\n",
    "            # 4.2.4 현재 위치를 복구\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 상태가치함수를 구하는 함수\n",
    "\n",
    " - 1개의 연결된 다음 상태를 참고하는 것부터 시작해서 2개의 연결된 다음 상태를 참고하는 식으로 13개의 연결된 다음 상태를 참고하도록 프로그램을 진행\n",
    " \n",
    " - max_step을 0부터 12까지 늘려가면서 **연결된 다음 상태를 몇 개 참고하느냐에 따라 각 상태의 상태가치가 어떻게 변하는지** 알아봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_step = 0 일 때 => 자기 상태와 연결된 다음 상태만의 보상의 합을 계산하는 모습\n",
    "- max_step = 1 일 때 => 자기 상태 + 연결된 다음 상태의 보상들의 합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -2.00      |      -1.50      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -1.50      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.58      |      -2.40      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -2.40      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.69      |      -3.16      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.16      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 3 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.53      |      -3.75      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.75      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 4 total_time = 0.01(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -6.18      |      -4.21      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.21      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 5 total_time = 0.03(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -6.67      |      -4.56      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.56      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 6 total_time = 0.12(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.05      |      -4.83      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.83      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 7 total_time = 0.49(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.35      |      -5.03      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.03      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 8 total_time = 1.71(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.57      |      -5.19      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.19      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 9 total_time = 5.51(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.74      |      -5.32      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.32      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEHCAYAAACk6V2yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdm0lEQVR4nO3de5RU9Znu8e/bzaVpEEVuikg3hnghUURaARE1JJPDZDgmOWgMgpnxQh+NJhIn8RJiJlkJkuWcOCbGM0tANGoPkRjGZEUmR0WrmjtyU1AwKkJPAwpIAKGBvr3nj67GBvrevetXVf181qpl967Lfqjok81bu37b3B0REck8WaEDiIhINFTwIiIZSgUvIpKhVPAiIhlKBS8ikqE6hQ5QV58+fTw/Pz90DBGRtLFmzZo97t63vvtSquDz8/NZvXp16BgiImnDzLY1dJ9GNCIiGUoFLyKSoVTwIiIZKqVm8CIiHVlFRQWlpaUcOXLkpPtycnIYOHAgnTt3bvbrqeBFRFJEaWkpp5xyCvn5+ZjZse3uzscff0xpaSmDBw9u9utpRCMiEkhRURH5+flkZWWRn5/Pnj176N2793HlDmBm9O7du94j+8boCF5EJICioiIKCwspKysDYNu2bezdu5e9e/fSu3fvkx5/Yuk3h47gRUQCmD59+rFyr+XubN++vd32oYIXEQmgpKSk3u3l5eXttg8VvIhIAIMGDTppW3V1dYNnybTm4kwqeBGRAGbMmEFOTs5x2z744AN69ux5UpnXnkVz4uObog9ZRUQCmDx5MosWLeLJJ5/EzBg0aNCxD1c3b9580uNrz4NvCRW8iEgghw8fZsCAAZSWlrbqLJmmaEQjIhKAuxOLxbjqqqsiKXdQwYuIBPHuu+/y4YcfcvXVV0e2DxW8iEgAsVgMgKuuuiqyfajgRUQCiMfjnHHGGZx77rmR7UMFLyKSZMmYv4MKXkQk6d5//3127NgR6fwdVPAiIklXO39XwYuIZJh4PE7//v0577zzIt2PCl5EJImSNX8HFbyISFJt2bKF0tLSSE+PrBXpUgVmthX4BKgCKt29IMr9iYikung8DkQ/f4fkrEXzBXffk4T9iIikvFgsRt++fbngggsi35dGNCIiSeLuxOPxpMzfIfqCd+AlM1tjZoUR70tEJKVt3bqVkpKSpMzfIfoRzRh332Fm/YCXzWyzuxfXfUCi+Auh/iuciIhkimTO3yHiI3h335H45y7gP4HL6nnMLHcvcPeCvn37RhlHRCSoWCxGnz59GDp0aFL2F1nBm1l3Mzul9mfgy8DGqPYnIpLq4vE4V155JVlZyfn4M8q99AeWmNkbwCrgRXf/S4T7ExFJWdu2bWPr1q1JG89AhDN4d98CDIvq9UVE0knt/D1ZH7CCTpMUEUmKWCzG6aefzuc///mk7VMFLyKSBLFYLKnzd1DBi4hErqSkhA8++CCp83dQwYuIRC7E/B1U8CIikYvH4/Tq1YuLLrooqftVwYuIRCzE/B1U8CIikSotLeX9999P+ngGVPAiIpFK9vozdangRUQiFI/HOfXUU5M+fwcVvIhIpGrn79nZ2UnftwpeRCQiO3bs4N133w0yfwcVvIhIZELO30EFLyISmVgsRs+ePbn44ouD7F8FLyISkXg8ztixY4PM30EFLyISiZ07d/LOO+8EG8+ACl5EJBLFxTWXnw71ASuo4EVEIhGLxTjllFMYPnx4sAwqeBGRCMTjca644go6dYrswnlNUsGLiLSzjz76iE2bNgWdv4MKXkSk3YVa//1EKngRkXYWj8fp0aMHl1xySdAcKngRkXYWi8UYM2YMnTt3DppDBS8i0o527drF22+/HXz+Dip4EZF2VXv+uwpeRCTDxONxunfvzogRI0JHUcGLiLSnVJm/gwpeRKTd7Nmzh40bNwY/PbJW5AVvZtlmts7M/hz1vkREQkql+Tsk5wj+LmBTEvYjIhJULBajW7duFBQUhI4CRFzwZjYQ+AdgTpT7ERFJBfF4nDFjxtClS5fQUYDoj+AfAe4Bqht6gJkVmtlqM1u9e/fuiOOIiETj448/5s0330yZ+TtEWPBmNgHY5e5rGnucu89y9wJ3L+jbt29UcUREIrV48WIgdebvEO0R/BjgGjPbCvwOGGdmz0a4PxGRYGrn75deemnoKMdEVvDufr+7D3T3fOCbwKvuPiWq/YmIhBSPxxk9ejRdu3YNHeUYnQcvItJGf/vb33jjjTdSajwDkJRLjbh7DIglY18iIsm2ePFi3D2lPmAFHcGLiLRZLBYjJyeHyy67LHSU46jgRUTaKBaLMWrUKHJyckJHOY4KXkSkDfbt28f69etTbv4OKngRkTZJ1fk7qOBFRNokHo/TtWtXRo0aFTrKSVTwIiJtkKrzd1DBi4i02v79+1m3bl1KjmdABS8i0mpLliyhuro6JT9gBRW8iEirxeNxunTpkpLzd1DBi4i0WiwWY+TIkXTr1i10lHqp4EVEWuHAgQOsWbMmZefvoIIXEWmVpUuXpvT8HVTwIiKtEovF6Ny5M6NHjw4dpUEqeBGRVojH41x22WXk5uaGjtIgFbyISAt98sknrF69OqXHM6CCFxFpsWXLllFVVZXSH7CCCl5EpMVisRidOnXi8ssvDx2lUSp4EZEWisfjXHrppXTv3j10lEap4EVEWuDgwYO8/vrrKT9/BxW8iEiLLFu2jMrKypSfv4MKXkSkReLxONnZ2YwZMyZ0lCap4EVEWiAWi1FQUECPHj1CR2mSCl5EpJkOHTqUNvN3UMGLiDTb8uXLqaioSIv5O6jgRUSaLZ3m7wCdmnqAmeUAE4CxwADgMLAReNHd34o2nohI6ojFYowYMYKePXuGjtIsjR7Bm9lPgKXAaGAl8DgwH6gEfmFmL5vZRVGHFBEJraysjFWrVqXNeAaaPoJ/3d1/0sB9D5tZP2BQfXcmjvyLga6J/Tzv7v/S2qAiIiGtWLGC8vLytPmAFZooeHd/8cRtZpYF9HD3A+6+C9jVwNOPAuPc/aCZdQaWmNl/ufuKNqcWEUmyWCxGVlYWV1xxRegozdasD1nN7D/MrKeZdQfeBt4xsx809hyvcTDxa+fEzduUVkQkkHg8ziWXXJI283do/lk0Q939APA1YCE1Y5kbm3qSmWWb2XpqjvJfdveV9Tym0MxWm9nq3bt3Nzu4iEiyHD58mBUrVqTV/B2aX/CdE2OWrwF/dPcKmnE07u5V7n4xMBC4zMw+X89jZrl7gbsX9O3bt/nJRUSSZOXKlWk3f4fmF/zjwFagO1BsZnnAgebuxN33ATFgfMviiYiEF4vFMLO0mr9DMwve3X/t7me5+1fc3YES4AuNPcfM+prZaYmfuwFfAja3Ma+ISNLF43GGDx/OaaedFjpKizR1HvyUxFkzx0l8gFppZp8xs4b+L+1M4DUzexN4nZoZ/J/bHllEJHmOHDnC8uXL0248A02fB98bWGdma4A1wG4gBxgCXAXsAe6r74nu/iYwvP2iiogk36pVqzh69GjafcAKTZ8H/ysz+w0wDhgDXETNUgWbgBvdvST6iCIi4dTO38eOHRs6Sos1uRaNu1cBLyduIiIdSjweZ9iwYfTq1St0lBZr7hedzjWzRWa2MfH7RWb2o2ijiYiEdfToUZYtW5aW83do/mmSs4H7gQo4Nl//ZlShRERSwapVqzhy5Ehazt+h+QWf6+6rTthW2d5hRERSSTwex8y48sorQ0dpleYW/B4z+wyJb6+a2bXAzshSiYikgFgsxoUXXsjpp58eOkqrNLfg76Dm26znm9l2YBpwe1ShRERCKy8vT+v5OzTjLBoAd98CfCmxmmSWu38SbSwRkbBef/11Dh8+nPkFn1hy4FtAPtDJzABw9+9GFUxEJKR4PA6Qlue/12pWwVOzRPAKYANQHV0cEZHUUDt/79OnT+gordbcgs9x97sjTSIikiIqKipYunQpN998c+gobdLcD1mfMbOpZnammZ1ee4s0mYhIIKtXr6asrCyt5+/Q/CP4cuBfgel8eqEPB86JIpSISEixWAwgbc9/r9Xcgr8bGOLue6IMIyKSCuLxOJ/73OdI96vMNXdE8xZQFmUQEZFUUFFRwZIlS9J2eYK6mnsEXwWsN7PXgKO1G3WapIhkmrVr13Lo0KG0n79D8wv+hcRNRCSj1c7fO8wRvLv/NuogIiKpIB6Pc8EFF9CvX7/QUdqs0YI3s/nu/g0z28CnZ8/UcncfFl00EZHkqqysZPHixdx4442ho7SLpo7g70r8cxPwgzrbDXgokkQiIoGsW7eOgwcPZsR4Bpq+JmvtksBD3H1b3fvM7PzIUomIBJBJ83doekRzO/Bt4Bwze7POXacAS6MMJiKSbLFYjPPOO48zzjgjdJR20dSI5j+A/wJmAvfV2f6Ju++NLJWISJJVVlayZMkSJk2aFDpKu2lqRLMf2A9kzp9YRKQe69ev58CBAxkznoHmf5NVRCSj1a7/roIXEckwsViMc889lwEDBoSO0m5U8CLS4VVVVbF48eKMOnqHCAvezM42s9fMbJOZvWVmdzX9LBGR5HvjjTfYv39/Rqw/U1dz16JpjUrgn919rZmdAqwxs5fd/e0I9yki0mKZOH+HCI/g3X2nu69N/PwJNd+GPSuq/YmItFYsFmPIkCGcdVZmVVRSZvBmlg8MB1bWc1+hma02s9W7d+9ORhwRkWOqqqooLi7OuKN3SELBm1kP4A/ANHc/cOL97j7L3QvcvSDdr54iIulnw4YN7Nu3L+Pm7xBxwZtZZ2rKvcjdF0S5LxGR1si09WfqivIsGgOeADa5+8NR7UdEpC3i8TjnnHMOZ599dugo7S7KI/gxwI3AODNbn7h9JcL9iYi0SHV1NcXFxRk5noEIT5N09yXUrBsvIpKSNm7cyN69ezNyPAP6JquIdGCZPH8HFbyIdGDxeJz8/Hzy8vJCR4mECl5EOpyioiLy8vJYsGABu3fvpqioKHSkSES5VIGISMopKiqisLCQsrIyAA4dOkRhYSEAkydPDhmt3ekIXkQ6lOnTpx8r91plZWVMnz49UKLoqOBFpEMpKSlp0fZ0poIXkQ6loQtqDxo0KMlJoqeCF5EO4+DBg9R8yf54ubm5zJgxI0CiaKngRaRDcHemTp3Khx9+yP33309eXh5mRl5eHrNmzcq4D1hBZ9GISAfx2GOP8bvf/Y4ZM2bwwx/+kAcffDB0pMjpCF5EMt6KFSu4++67mTBhAvfdd1/oOEmjgheRjLZ7926uu+46Bg4cyNNPP01WVsepPY1oRCRjVVVVccMNN7B7926WLVtGr169QkdKKhW8iGSsn/70p7zyyivMnj2bSy65JHScpOs4f1cRkQ5l4cKF/OxnP+Omm27illtuCR0nCBW8iGScrVu3MmXKFIYNG8Zjjz1W77nvHYEKXkQyypEjR7j22muprq7m+eefp1u3bqEjBaMZvIhklGnTprFmzRpeeOEFhgwZEjpOUDqCF5GM8fTTT/P4449z77338tWvfjV0nOBU8CKSETZs2MBtt93G1Vdfzc9//vPQcVKCCl5E0t7+/fuZOHEip512GvPmzaNTJ02fQTN4EUlz7s7NN9/Mli1beO211xpcDrgjUsGLSFp7+OGHWbBgAb/85S8ZO3Zs6DgpRSMaEUlbxcXF3HvvvUycOJHvfe97oeOkHBW8iKSlDz/8kOuvv55zzjmHuXPndtgvMzVGIxoRSTuVlZVcf/317N+/n5deeomePXuGjpSSVPAiknamT59OcXExzzzzDBdeeGHoOCkrshGNmc01s11mtjGqfYhIx/PCCy/w0EMPcdtttzFlypTQcVJalDP4p4DxEb6+iHQw7733Hv/4j/9IQUEBjzzySOg4KS+ygnf3YmBvVK8vIh1LWVkZEydOpFOnTjz//PN07do1dKSUF3wGb2aFQCHAoEGDAqcRkVTk7txxxx1s2LCBhQsXkpeXFzpSWgh+mqS7z3L3Ancv6Nu3b+g4IpKCnnjiCZ566ikeeOABxo/X5Le5ghe8iEhj1q5dy5133smXv/xlfvzjH4eOk1ZU8CKSsvbu3cvEiRPp168fRUVFZGdnh46UVqI8TXIesBw4z8xKzaxjXhRRRFqlurqab33rW2zfvp3f//739OnTJ3SktBPZh6zuPimq1xaRzPeLX/yCF198kd/85jeMHDkydJy0pBGNiKScRYsW8cADD3DDDTfw7W9/O3SctKWCF5GUUlpayqRJkzj//POZNWuWFhFrAxW8iKSM8vJyvvGNb3D48GH+8Ic/0L1799CR0lrwLzqJiNS65557WL58OfPnz+f8888PHSft6QheRFLCc889x69+9SumTZvGddddFzpORlDBi0hwmzZt4tZbb+Xyyy/noYceCh0nY6jgRSSogwcPMnHiRLp168b8+fPp3Llz6EgZQzN4EQnG3SksLOSdd97h5Zdf5qyzzgodKaOo4EUkmMcee4x58+bx4IMPMm7cuNBxMo5GNCKSVEVFReTn55OVlcV3vvMdhg8fzr333hs6VkZSwYtI0hQVFVFYWMi2bdtwdwA2b97MvHnzAifLTCp4EUma6dOnU1ZWdty2w4cPM3369ECJMptm8CISOXdn5cqVbNu2rd77S0pKkpyoY9ARvIhEZu/evfz617/moosuYvTo0Q2uK6PLdUZDBS8i7crdicfjTJkyhQEDBnDXXXeRk5PD448/zuzZs8nNzT3u8bm5ucyYMSNQ2symEY2ItItdu3bx29/+ljlz5vDXv/6Vnj17cssttzB16lQuvvjiY4/Lyclh+vTplJSUMGjQIGbMmMHkyZPDBc9gVvtJdiooKCjw1atXh44hIs1UXV3NK6+8wuzZs/njH/9IRUUFY8aMYerUqVx33XUnHa1L+zOzNe5eUN99OoIXkRbbsWMHTz75JHPmzGHr1q307t2bO++8k1tvvZWhQ4eGjicJKngRaZbKykr+8pe/MHv2bF588UWqqqoYN24cM2fO5Otf/zpdu3YNHVFOoIIXkUZt27aNJ554grlz57J9+3b69+/PD37wA2655RaGDBkSOp40QgUvIiepqKjgT3/6E7Nnz+all14CYPz48Tz66KNMmDBBKz6mCRW8iBzz7rvvMmfOHJ566il27drFwIEDeeCBB7j55pvJy8sLHU9aSAUv0sEdOXKEBQsWMHv2bGKxGNnZ2UyYMIGpU6cyfvx4srOzQ0eUVlLBi3QQRUVFx51/fvvtt7Nz506eeeYZ9u7dy+DBg5kxYwY33XQTZ555Zui40g50HrxIB1C7iuOJC31lZWVx7bXXMnXqVMaNG0dWlr7cnm50HrxIBquqqmLPnj189NFHDd5effVVKioqTnrugAEDeO655wKklmRQwYtE7MTRSHO+ml9ZWcmuXbsaLe3a2549e6iurj7pNbp27Ur//v3p379/veUOsH379nb5M0qKcvfIbsB44B3gPeC+ph4/YsQIb6lnn33W8/Ly3Mw8Ly/Pn3322Ra/RntIhRypkEE5Ts6Qm5vrwLFbTk6Of//73/e5c+f6zJkzfdq0aT5p0iQfN26cDx061Hv37n3c4+veunXr5vn5+T5y5Ei/5pprfOrUqf6jH/3IH330UZ8/f77H43HfvHmz79u3z6urq4/lyMvLq/f18vLykv6eSPsCVnsDnRrZDN7MsoG/An8HlAKvA5Pc/e2GntPSGXx9c8Xc3FxmzZqV1MWLUiFHKmRoSw53p6qq6titurq6wd+bc9/ChQuZOXMmR48ePbaPrl27cvvttzNq1CgqKiooLy8/7p/N3daSx+/cubPeo+u6evTocexIu6lbjx49GlxyN4r/XST1NTaDj7LgRwM/cff/kfj9fgB3n9nQc1pa8Pn5+fVeQKBr166MGTOmxZlba+nSpccVSd0co0ePPvZ73ff6xPe9Jb/Xd9+aNWsoLy8/KUOXLl0YNmxYfX+7auhvXW3aXlpaSlVV1Uk5srKyOPXUUxss5lTTpUsXOnfufOyfdX9uyba5c+fW+/pmxvvvv0+/fv3o3r17Uv5MrRkVSeoL9SHrWcB/1/m9FBh54oPMrBAohJYv+t/QVWCOHj1ab9lFpb5yr91+4tFb7dGXmZ10JNaS30+8r6E/b3l5Ob179z62v7q3ujlau/3EbU8//XS9Oaqrq5kyZQpZWVlkZ2cfu9X9vbX31ffYa6655qT/I6zNunHjxkZLOjs7u1VHyfVZtGhRvQchgwYNYvDgwe2yj+aaPHmyCr2jaWh209YbcB0wp87vNwKPNvacls7gU2WumAo5UiGDcpysvhl8bm5usM8lJPPQyAw+ypNeS4Gz6/w+ENjRnjuYMWNGSlwdJhVypEIG5TjZ5MmTmTVrFnl5eZgZeXl5mntL8jTU/G29UTP+2QIMBroAbwCfa+w5Oosm/TMoh0hyEeIsGgAz+wrwCJANzHX3Rg+f9E1WEZGWCfZNVndfCCyMch8iIlI/LTwhIpKhVPAiIhlKBS8ikqFU8CIiGSql1oM3s93AyV/7a54+wJ52jJPO9F4cT+/H8fR+fCoT3os8d+9b3x0pVfBtYWarGzpVqKPRe3E8vR/H0/vxqUx/LzSiERHJUCp4EZEMlUkFPyt0gBSi9+J4ej+Op/fjUxn9XmTMDF5ERI6XSUfwIiJShwpeRCRDpX3Bm9l4M3vHzN4zs/tC5wnJzM42s9fMbJOZvWVmd4XOFJqZZZvZOjP7c+gsoZnZaWb2vJltTvw7MrrpZ2UuM/te4r+TjWY2z8xyQmdqb2ld8IkLez8G/D0wFJhkZkPDpgqqEvhnd78AGAXc0cHfD4C7gE2hQ6SIXwF/cffzgWF04PfFzM4CvgsUuPvnqVnS/JthU7W/tC544DLgPXff4u7lwO+ArwbOFIy773T3tYmfP6HmP+CzwqYKx8wGAv8AzAmdJTQz6wlcCTwB4O7l7r4vaKjwOgHdzKwTkEs7X3EuFaR7wdd3Ye8OW2h1mVk+MBxYGThKSI8A9wDVTTyuIzgH2A08mRhZzTGz7qFDheLu24H/A5QAO4H97v5S2FTtL90L3urZ1uHP+zSzHsAfgGnufiB0nhDMbAKwy93XhM6SIjoBlwD/7u7DgUNAh/3Mysx6UfO3/cHAAKC7mU0Jm6r9pXvBR35h73RjZp2pKfcid18QOk9AY4BrzGwrNaO7cWb2bNhIQZUCpe5e+ze656kp/I7qS8AH7r7b3SuABcDlgTO1u3Qv+NeBz5rZYDPrQs2HJH8KnCkYMzNqZqyb3P3h0HlCcvf73X2gu+dT8+/Fq+6ecUdozeXuHwL/bWbnJTZ9EXg7YKTQSoBRZpab+O/mi2Tgh86RXpM1au5eaWZ3Av+PTy/s/VbgWCGNAW4ENpjZ+sS2HyaujSvyHaAocTC0BbgpcJ5g3H2lmT0PrKXm7LN1ZOCyBVqqQEQkQ6X7iEZERBqgghcRyVAqeBGRDKWCFxHJUCp4EZEMpYIXEclQKniRVjCzrWbWJ3QOkcao4EVEMpQKXtKameUnLmAxJ3HhhiIz+5KZLTWzd83sssRtWWIVxWW1X9c3s7vNbG7i5wsTz89tYD+9zeylxGs8Tp2F7hKvszFxm5bYdo+ZfTfx87+Z2auJn79YuyaOmR00sxlm9oaZrTCz/lG+V9LxqOAlEwyh5mIWFwHnAzcAVwDfB34IbAauTKyi+GPgwcTzHgGGmNnXgSeB/+3uZQ3s41+AJYnX+BMwCMDMRlDzlf+R1FxkZaqZDQeKgbGJ5xYAPRILwV0BLE5s7w6scPdhicdPbdvbIHK8tF6LRiThA3ffAGBmbwGL3N3NbAOQD5wK/NbMPkvNctKdAdy92sz+CXgTeNzdlzayjyuB/5V43otm9rfE9iuA/3T3Q4n9L6Cm2P8dGGFmpwBHqVnzpCBx33cTzy0Hai8luAb4u7a8CSIn0hG8ZIKjdX6urvN7NTUHMT8DXktcmu1/AnWvvflZ4CA1a4I3pb6Fm+q7JgGJJWi3UnN0v4yao/YvAJ/h01ULK/zTxaCq0AGXtDMVvHQEpwLbEz//U+1GMzuVmtHOlUBvM7u2kdcoBiYnnvf3QK8627+WWHa2O/B1Ph3BFFMzJipObLsNWO9a4U+SRAUvHcFDwEwzW0rNstK1/g34v+7+V+AW4Bdm1q+B1/gpcKWZrQW+TM164iSugfsUsIqayyPOcfd1iecsBs4Elrv7R8ARPi1/kchpuWARkQylI3gRkQylD3VE6jCzm4C7Tti81N3vCJFHpC00ohERyVAa0YiIZCgVvIhIhlLBi4hkKBW8iEiG+v80ksP1DOkS6gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "# 3. 최대 max_step_number 제한\n",
    "max_step_number = 10\n",
    "\n",
    "# 4. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "# 5. 재귀함수 state_value_function을를 이용해 각 상태 가치를 계산\n",
    "for max_step in range(max_step_number):\n",
    "    \n",
    "    # 5.1 미로 각 상태의 가치를 테이블 형식으로 저장\n",
    "    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5.2 미로의 각 상태에 대해 state_value_function() 을 이용해 가치를 계산한 후 테이블 형식으로 저장\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i,j])\n",
    "            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n",
    "            \n",
    "    # 5.3 max_down에 따른 계산시간 저장\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "    \n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 6. step 별 계산 시간 그래프 그리기    \n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 코드까지는 각 상태에서 최대 스텝 제한을 두어 벨만 방정식을 따른 상태 가치 함수의 근사값을 구하는 과정이다.\n",
    "\n",
    "상태는 (0, 0), (0, 1), (1, 0), (1, 1)이 있다.\n",
    "\n",
    "행동은 상, 하, 좌, 우 이동으로 각 확률은 25%로 균등하다.\n",
    "\n",
    "보상은 road로 가는 행동은 -1, goal로 가는 행동은 1이며, 맵 바깥을 벗어나거나 절벽으로 가는 행동은 -3 이다.\n",
    "\n",
    "맵 바깥을 벗어나거나 절벽으로 가면 행동하기 전 위치로 상태가 된다.\n",
    "\n",
    "\n",
    "특정 상태에서의 벨만 방정식 상태 가치 함수를 구할 때는, (엄밀하게 정의하자면) 무한한 행동을 취했을 때의 보상의 총합의 기댓값이다. 실제로는 무한한 행동을 구현할 수는 없으므로, 이 코드에서는 max_step_number로 행동 횟수 제한을 두었다 (이 코드에서 max_step_number=0 이면 행동 1번만 고려한다는 의미이다).\n",
    "\n",
    "max_step_number가 크면 클 수록 엄밀한 벨만방정식 상태 가치 함수에 가까워질 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 1-1. 위 코드 내용을 기반으로 상태는 (0, 0), (0, 1), (1, 0), (1, 1)에 대한 마르코프 체인 네트워크와 상태 전이 매트릭스를 그려보자. 옳게 그린다면, 상태 전이 매트릭스는 4x4 행렬이어야 한다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 1-2. (1.)에서 그린 상태 전이 매트릭스를 기반으로, max_step_number = 0일 때 (즉, 행동 한 번만 고려할 때) 상태 (0, 0), (0, 1), (1, 0)에서에 벨만 방정식 상태 가치 함수의 근사값을 수식으로 구하는 과정을 보여보자.\n",
    "\n",
    "##### 왜 0.5를 곱한거지 ...? 0.0은 뭐야 ?\n",
    "\n",
    "예: V(X=(0, 0)) = P_(0,0)(0,0) * R_(0,0) + P_(0,0)(0,1) * R_(0,1) + P_(0,0)(1,0) * R_(1,0) + P_(0,0)(1,1) * R_(1,1) \n",
    "= 0.5 * -3 + 0.25 * -1 + 0.25 * -1 + 0.0 * 1 = -2.00\n",
    "\n",
    "- V(X=(0, 1)) = P_(0,1)(0,0) * R_(0,0) + P_(0,1)(0,1) * R_(0,1) + P_(0,1)(1,0) * R_(1,0) + P_(0,1)(1,1) * R_(1,1) = \n",
    "- V(X=(1, 0)) = P_(1,0)(0,0) * R_(0,0) + P_(1,0)(0,1) * R_(0,1) + P_(1,0)(1,0) * R_(1,0) + P_(1,0)(1,1) * R_(1,1) = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 행동가치함수를 계산하는 함수\n",
    "\n",
    "- 인자에 에이전트가 선택한 행동 act가 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 가치 함수\n",
    "def action_value_function(env, agent, act, G, max_step, now_step):   \n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. 현재 위치가 목적지인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "\n",
    "    # 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n",
    "    else:\n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += reward\n",
    "        \n",
    "        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "        if done == True:            \n",
    "            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                agent.set_pos(pos1)\n",
    "            \n",
    "        # 4.3 현재 위치를 다시 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 행동의 가치를 계산\n",
    "\n",
    "- 가능한 4가지 행동(위, 오른쪽, 아래, 왼쪽), 9개의 모든 상태에 대해 max_step=7만큼 연결된 다음 상태들의 보상을 참고해 행동가치함수를 계산\n",
    "- 각각의 방향으로 뻗어나갔을 때, 나타나는 행동의 가치를 표현 -> 거기서 가장 큰 값들을 화살표로 표시\n",
    "- 결국, 모든 상태에서 가능한 행동들의 행동가치를 계산할 수 있다면, 현재 상태에서 최적의 행동을 선택하는 것이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -0.75       |     -0.75       |\n",
      "| -0.75     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |      0.25       |\n",
      "+-----------------+-----------------+\n",
      "|     -0.25       |      1.00       |\n",
      "| -0.75      0.25 |  1.00      1.00 |\n",
      "|     -0.75       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -3.45       |     -3.34       |\n",
      "| -3.45     -1.34 | -1.45     -3.34 |\n",
      "|     -1.34       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -1.45       |      1.00       |\n",
      "| -3.34      1.90 |  1.00      1.00 |\n",
      "|     -3.34       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -5.15       |     -4.40       |\n",
      "| -5.15     -2.40 | -3.15     -4.40 |\n",
      "|     -2.40       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -3.15       |      1.00       |\n",
      "| -4.40      1.90 |  1.00      1.00 |\n",
      "|     -4.40       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 3 total_time = 0.01(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -6.40       |     -5.26       |\n",
      "| -6.40     -3.26 | -4.40     -5.26 |\n",
      "|     -3.26       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -4.40       |      1.00       |\n",
      "| -5.26      1.90 |  1.00      1.00 |\n",
      "|     -5.26       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 4 total_time = 0.02(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -7.35       |     -5.93       |\n",
      "| -7.35     -3.93 | -5.35     -5.93 |\n",
      "|     -3.93       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.35       |      1.00       |\n",
      "| -5.93      1.90 |  1.00      1.00 |\n",
      "|     -5.93       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 5 total_time = 0.07(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -8.08       |     -6.44       |\n",
      "| -8.08     -4.44 | -6.08     -6.44 |\n",
      "|     -4.44       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.08       |      1.00       |\n",
      "| -6.44      1.90 |  1.00      1.00 |\n",
      "|     -6.44       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 6 total_time = 0.16(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -8.63       |     -6.84       |\n",
      "| -8.63     -4.84 | -6.63     -6.84 |\n",
      "|     -4.84       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.63       |      1.00       |\n",
      "| -6.84      1.90 |  1.00      1.00 |\n",
      "|     -6.84       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 7 total_time = 0.66(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.06       |     -7.14       |\n",
      "| -9.06     -5.14 | -7.06     -7.14 |\n",
      "|     -5.14       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.06       |      1.00       |\n",
      "| -7.14      1.90 |  1.00      1.00 |\n",
      "|     -7.14       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 8 total_time = 2.14(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.39       |     -7.38       |\n",
      "| -9.39     -5.38 | -7.39     -7.38 |\n",
      "|     -5.38       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.39       |      1.00       |\n",
      "| -7.38      1.90 |  1.00      1.00 |\n",
      "|     -7.38       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 9 total_time = 6.45(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.65       |     -7.56       |\n",
      "| -9.65     -5.56 | -7.65     -7.56 |\n",
      "|     -5.56       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.65       |      1.00       |\n",
      "| -7.56      1.90 |  1.00      1.00 |\n",
      "|     -7.56       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 재귀적으로 행동의 가치를 계산\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "max_step_number = 10\n",
    "\n",
    "for max_step in range(max_step_number):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n",
    "    #print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            \n",
    "            # 4.2 모든 행동에 대해\n",
    "            for action in range(len(agent.action)):\n",
    "                \n",
    "                # 4.2.1 에이전트의 위치를 초기화\n",
    "                agent.set_pos([i,j])\n",
    "                \n",
    "                # 4.2.2 현재 위치에서 행동 가치를 계산\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "                \n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 평가를 통한 행동 가치 함수 구하기\n",
    "\n",
    "- 정책(Policy)는 어떤 상태에서 행동을 선택할 확률\n",
    "- 최적 정책은 그 중에서도 어떤 상태에서의 최적의 행동을 알려주는 정책\n",
    "    - 이 값을 찾기 위해, 여러 가지 정책을 따르는 가치 함수를 찾고, 가치 함수들로부터 최적의 가치 함수를 찾으면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "+-----------------+-----------------+\n",
      "|     -3.00       |     -3.00       |\n",
      "| -3.00     -1.00 | -1.00     -3.00 |\n",
      "|     -1.00       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -1.00       |      1.00       |\n",
      "| -3.00      1.90 |  1.00      1.00 |\n",
      "|     -3.00       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 1\n",
      "+-----------------+-----------------+\n",
      "|     -4.80       |     -4.15       |\n",
      "| -4.80     -2.15 | -2.80     -4.15 |\n",
      "|     -2.15       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -2.80       |      1.00       |\n",
      "| -4.15      1.90 |  1.00      1.00 |\n",
      "|     -4.15       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 2\n",
      "+-----------------+-----------------+\n",
      "|     -6.13       |     -5.07       |\n",
      "| -6.13     -3.07 | -4.13     -5.07 |\n",
      "|     -3.07       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -4.13       |      1.00       |\n",
      "| -5.07      1.90 |  1.00      1.00 |\n",
      "|     -5.07       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 3\n",
      "+-----------------+-----------------+\n",
      "|     -7.14       |     -5.78       |\n",
      "| -7.14     -3.78 | -5.14     -5.78 |\n",
      "|     -3.78       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.14       |      1.00       |\n",
      "| -5.78      1.90 |  1.00      1.00 |\n",
      "|     -5.78       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 4\n",
      "+-----------------+-----------------+\n",
      "|     -7.91       |     -6.33       |\n",
      "| -7.91     -4.33 | -5.91     -6.33 |\n",
      "|     -4.33       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.91       |      1.00       |\n",
      "| -6.33      1.90 |  1.00      1.00 |\n",
      "|     -6.33       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 5\n",
      "+-----------------+-----------------+\n",
      "|     -8.51       |     -6.75       |\n",
      "| -8.51     -4.75 | -6.51     -6.75 |\n",
      "|     -4.75       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.51       |      1.00       |\n",
      "| -6.75      1.90 |  1.00      1.00 |\n",
      "|     -6.75       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 6\n",
      "+-----------------+-----------------+\n",
      "|     -8.97       |     -7.08       |\n",
      "| -8.97     -5.08 | -6.97     -7.08 |\n",
      "|     -5.08       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.97       |      1.00       |\n",
      "| -7.08      1.90 |  1.00      1.00 |\n",
      "|     -7.08       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 7\n",
      "+-----------------+-----------------+\n",
      "|     -9.32       |     -7.32       |\n",
      "| -9.32     -5.32 | -7.32     -7.32 |\n",
      "|     -5.32       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.32       |      1.00       |\n",
      "| -7.32      1.90 |  1.00      1.00 |\n",
      "|     -7.32       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 8\n",
      "+-----------------+-----------------+\n",
      "|     -9.59       |     -7.52       |\n",
      "| -9.59     -5.52 | -7.59     -7.52 |\n",
      "|     -5.52       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.59       |      1.00       |\n",
      "| -7.52      1.90 |  1.00      1.00 |\n",
      "|     -7.52       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 9\n",
      "+-----------------+-----------------+\n",
      "|     -9.80       |     -7.66       |\n",
      "| -9.80     -5.66 | -7.80     -7.66 |\n",
      "|     -5.66       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.80       |      1.00       |\n",
      "| -7.66      1.90 |  1.00      1.00 |\n",
      "|     -7.66       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def update_q_table_once(env, agent, q_table):   \n",
    "    old_q_table = q_table\n",
    "    old_v_table = np.zeros([env.reward.shape[0], env.reward.shape[1]])\n",
    "    new_q_table = np.copy(q_table)\n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"goal\":\n",
    "                old_v_table[i][j] = env.goal\n",
    "            else:\n",
    "                for action in range(len(agent.action)):\n",
    "                    old_v_table[i][j] +=  agent.select_action_pr[action] * old_q_table[i][j][action]\n",
    "    \n",
    "    \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for action in range(len(agent.action)):\n",
    "                if env.reward_list1[i][j] == \"goal\":\n",
    "                    new_q_table[i][j][action] = env.goal\n",
    "                    continue\n",
    "                agent.set_pos([i, j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                next_pos = agent.get_pos()\n",
    "                new_q_table[i][j][action] = reward + gamma * old_v_table[next_pos[0], next_pos[1]]\n",
    "    return new_q_table\n",
    "\n",
    "  \n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"step {}\".format(i))\n",
    "    q_table = update_q_table_once(env, agent, q_table)\n",
    "  \n",
    "    show_q_table(np.round(q_table, 2), env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(np.round(q_table, 2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 코드까지는 각 (상태, 행동) 쌍에서 행동 가치 함수를 구하는 두 가지 과정이다.\n",
    "\n",
    "### 질문 2-1. 재귀적으로 구하는 과정과 정책 평가로 구하는 두 과정을 비교해보자. 구하는 과정은 어떤 점에서 차이가 있는가? 두 행동 가치 함수 테이블은 가까워 지는가? 무엇이 더 좋은 방법이라고 생각하는가? 그 이유는 무엇인가?\n",
    "\n",
    "- 재귀적으로 구하는 과정은 가능한 모든 행동, 모든 상태에 대해 연결된 다음 상태들의 보상을 참고해 행동가치함수를 계산함. 그리고 각각의 방향으로 뻗어나갔을 때 나타나는 행동의 가치를 표현하며, 결국 모든 상태에서 가능한 행동들의 가치를 계산할 수 있다면, 현재 상태에서 최적의 행동을 선택하는 것이 가능함.\n",
    "\n",
    "- 정책 평가로 구하는 과정은 재귀와는 달리, 연결된 모든 상태를 재귀적으로 탐색하는 것이 아니라, 오직 연결된 다음 상태의 상태가치만을 이용해 상태가치를 계산하는 과정을 무한히 반복하는 것이다. 그리고 재귀보다 계산 시간도 훨씬 짧고, 상태가치도 수렴한다.\n",
    "\n",
    "- 결국, 재귀적으로 구하는 과정은 연결된 모든 상태를 재귀적으로 탐색하고, 정책 평가로 구하는 과정은 오직 연결된 다음 상태의 상태가치만을 이용하기에 계산 시간에 있어서 정책 평가가 훨씬 적게 걸린다. 하지만, 상태가치를 계산해 나가다보면, 상태가치도 어느새 수렴한다. 결국, 두 행동 가치 함수는 가까워진다고 볼 수 있다.\n",
    "\n",
    "- 그 중에서도 정책 평가로 구하는 과정이 더 좋은 방법이라고 생각한다. 재귀적인 방법도 물론 좋지만, 수많은 정책 중에서도 최적으로 평가된 정책 평가로 구하는 과정이 계산 시간이 짧아 더 데이터가 커진다면, 실행 간을 줄일 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 2-2. 왼쪽 상단, 즉 (0, 0)에서 에이전트가 출발한다고 생각해보자. 어떻게 행동을 취하면 Goal에 도착하는가? 가능한 행동의 리스트를 한 가지 경우만 제시해보자. 어떠한 기준으로 행동을 선택하였는가?\n",
    "\n",
    "- 재귀적으로 행동의 가치를 계산한다고 가정하면, (0, 0)에서 상태 가치가 높은 아래로 이동하고, (1, 0)에서 오른쪽으로 이동하여 Goal에 도착한다.\n",
    "- 이 선택을 하기에 앞서, 나는 모든 상태에서 가능한 상태 가치를 계산할 수 있기에, 현재 상태에서 최적의 행동을 선택했다. (사실 오른쪽 -> 아래로 가도 결과는 같다)\n",
    "\n",
    "- 정책 평가로 계산한다고 가정하면, 이 경우에도 위와 똑같이 행동을 할 것이다. 이 역시, 연결된 다음 상태의 상태가치가 가장 높은 것을 선택했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c638347e88009a0d44f6e45de08008756b57d34d065fea7b6e15509fa1413ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
