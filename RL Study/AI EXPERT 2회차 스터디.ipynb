{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 평가, 정책 개선\n",
    "\n",
    "여기서 policy 변수는 각 상태에 대하여 어떠한 행동을 취할 지 저장하는 행렬로 정의하였다.\n",
    "\n",
    "0, 1, 2, 3 = 위, 오른쪽, 아래, 왼쪽\n",
    "\n",
    "예:\n",
    "\n",
    "policy[0, 0] = 1 의 뜻 : 상태 (0, 0) 에서 오른쪽으로 이동하겠다\n",
    "\n",
    "policy[1, 2] = 3 의 뜻 : 상태 (1, 2) 에서 왼쪽으로 이동하게 하겠다.\n",
    "\n",
    "수식으로 표현하자면 policy[i, j] = a 라는 뜻은\n",
    "\n",
    "$$\\pi(a|s_{ij}) = 1, \\pi(a'|s_{ij}) = 0\\text{  }(a'\\in A, a'\\neq a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복 정책 평가\n",
    "\n",
    "\n",
    "* 구현 방법\n",
    "\n",
    "\n",
    "1. 상태가치를 저장하는 상태가치 테이블을 2개 이용하는 것: 이전에 계산된 상태가치를 이용해 새로운 상태가치를 계산해서 다른 테이블에 저장하고, 저장된 상태가치 테이블을 이전 상태가치가 저장된 테이블에 복사하고 다시 새로운 상태가치를 계산해서 테이블로 계산하는 방식\n",
    "\n",
    "\n",
    "2. 상태가치를 저장하는 상태가치 테이블을 1개 이용하는 것: 메모리를 줄일 수 있지만, k의 스텝이 맞지 않는 경우가 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* S: 마지막 상태를 포함하는 모든 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V table 갱신 함수\n",
    "def policy_evalution(env, agent, v_table, policy):\n",
    "    gamma = 0.9\n",
    "    \n",
    "    while(True):\n",
    "        # Δ←0\n",
    "        delta = 0\n",
    "        \n",
    "        #  v←𝑉(𝑠)\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "        \n",
    "        # 모든 𝑠∈𝑆에 대해 :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                \n",
    "                # 에이전트를 지정된 좌표에 위치시킨후 가치함수를 계산\n",
    "                agent.set_pos([i,j])\n",
    "                \n",
    "                # 현재 정책의 행동을 선택\n",
    "                action = policy[i,j]\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n",
    "                \n",
    "        # ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "        # 계산전과 계산후의 가치의 차이를 계산\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "                \n",
    "        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "        if delta < 0.000001:\n",
    "            break\n",
    "            \n",
    "    return v_table, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책개선\n",
    "\n",
    "\n",
    "* 개선: 각 상태의 상태가치 V(s)를 이용해 새로운 정책으로 개선하는 것\n",
    "\n",
    "\n",
    "* 정책 개선: 정책 평가를 통해 계산된 새로운 상태가치를 이용해 최적의 행동을 선택하는 것\n",
    "    - 최적의 행동: 행동가치가 가장 큰 행동\n",
    "    - 최적의 행동을 선택함으로써 현재 정책이 새로운 정책으로 개선됨\n",
    "    \n",
    "    \n",
    "* 즉, 과거의 정책으로 계산된 행동가치에 의해 새로운 정책으로 개선되는 과정을 '정책 개선'이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy 갱신 함수\n",
    "def policy_improvement(env, agent, v_table, policy):\n",
    "\n",
    "    gamma = 0.9  \n",
    "    \n",
    "    # policyStable ← true \n",
    "    policyStable = True\n",
    "\n",
    "    # 모든 s∈S에 대해：\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):   \n",
    "            \n",
    "            # 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛←π(s) \n",
    "            old_action = policy[i,j]          \n",
    "            \n",
    "            # 가능한 행동중 최댓값을 가지는 행동을 선택\n",
    "            temp_action = 0\n",
    "            temp_value =  -1e+10           \n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    temp_action = action\n",
    "                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n",
    "                    \n",
    "            # 만약 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛\"≠π(s)\"라면， \"policyStable ← False\" \n",
    "            # old-action과 새로운 action이 다른지 체크\n",
    "            if old_action != temp_action :\n",
    "                policyStable = False\n",
    "                \n",
    "            policy[i,j] = temp_action\n",
    "            \n",
    "    return policy, policyStable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 반복 (정책 평가 <=> 정책 개선)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prediction: 새로운 상태가치함수를 계산하는 과정\n",
    "* Control : 새로운 정책을 생성하는 과정\n",
    "\n",
    "\n",
    "* 정책 반복: Prediction과 Control을 반복하면서 최적의 가치함수 V와 최적의 정책 π를 찾아가는 알고리즘\n",
    "    - Prediction: 정책 평가를 사용해 현재 정책을 이용해 새로운 상태가치함수를 계산\n",
    "    - Control: Prediction에서 계산된 상태가치함수를 이용해 기존 정책을 새로운 정책으로 개선\n",
    "    - 계속 반복하다보면, 최적 가치함수와 최적 정책으로 수렴하게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.54      |       0.42      |       0.65      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.44      |       0.89      |       0.96      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial random Policy π0(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ←         |      ←         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      →         |      →         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "start policy iteration\n",
      "\n",
      "Vπ0(S) delta = 0.0000009713\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -28.00      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -30.00      |     -28.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|     -28.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π1(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Vπ1(S) delta = 0.0000002328\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       3.12      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π2(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Vπ2(S) delta = 0.0000001885\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       6.20      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "policy π3(S)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "total_time = 0.019945144653320312\n"
     ]
    }
   ],
   "source": [
    "# 정책 반복\n",
    "# 환경과 에이전트에 대한 초기 설정\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 1. 초기화\n",
    "# 모든 𝑠∈𝑆에 대해 𝑉(𝑠)∈𝑅과 π(𝑠)∈𝐴(𝑠)를 임의로 설정\n",
    "\n",
    "#shape : [h, w]\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "\n",
    "#shape : [h, w]\n",
    "#값 : 해당 상태에서 어떠한 행동을 취할 것인지 나타내는 정수\n",
    "policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"Initial random V(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "print(\"Initial random Policy π0(S)\")\n",
    "show_policy(policy,env)\n",
    "print(\"start policy iteration\")\n",
    "\n",
    "# 시작 시간을 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "max_iter_number = 20000\n",
    "for iter_number in range(max_iter_number):\n",
    "    \n",
    "    # 2.정책평가\n",
    "    v_table, delta = policy_evalution(env, agent, v_table, policy)\n",
    "\n",
    "    # 정책 평가 후 결과 표시                                            \n",
    "    print(\"\")\n",
    "    print(\"Vπ{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "    # 3.정책개선\n",
    "    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n",
    "\n",
    "    # policy 변화 저장\n",
    "    print(\"policy π{}(S)\".format(iter_number+1))\n",
    "    show_policy(policy,env)\n",
    "    # 하나라도 old-action과 새로운 action이 다르다면 '2. 정책평가'를 반복\n",
    "    if(policyStable == True):\n",
    "        break\n",
    "\n",
    "        \n",
    "print(\"total_time = {}\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`초기 정책은 모든 상태에서 상하좌우 중 임의의 행동(방향)으로 설정되어 있음. 저액 평가에서는 이 초기 정책을 이용해 각 상태에서 행동을 선택하고 상태가치를 계산함. 정책 평가가 끝나면 정책 개선에서는 모든 상태에서 계산된 상태가치를 이용해 각 상태에서 최적의 행동으로 구성된 개선된 정책을 계산. 정책 개선과 정책평가를 정책이 수렴할 때까지 반복`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 몬테카를로 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 프리 알고리즘(Model-free algorithm)\n",
    "\n",
    "\n",
    "- 동적계획법에서는 우리가 환경에 대한 정보를 완전히 파악하고 있다는 가정하에 가치와 정책을 계산했으므로, 최적 가치와 최적 정책을 계산할 수 있었음. 하지만, 실제환경은 그렇지 않음. 가능성이 있는 조건을 모두 열거할 수 없고, 그 조건에 따른 확률을 전혀 알 수 없음.\n",
    "    \n",
    "    \n",
    "- 대표적인 알고리즘: 몬테카를로 방법 (MC), 시간차 학습 (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몬테카를로 방법의 Prediction\n",
    "\n",
    "\n",
    "- 탐색적인 방법을 이용해 상태가치함수와 행동가치함수를 학습\n",
    "\n",
    "\n",
    "- 어디를 어떻게 이동해야하는지에 대한 상태전이확률 정보가 전혀 없음. 임의의 상태에서 임의의 정책을 이용해 행동을 선택하고 이동하는 시도를 무수히 반복하고, 그 결과를 이용해 에이전트가 알지 못하는 환경에 대해 최적의 행동을 학습하는 것\n",
    "\n",
    "\n",
    "- 수행 과정\n",
    "    1. 도착지점을 제외한 s0부터 s7까지 미로의 모든 상태에서 에피소드를 시작하고, 에피소드별로 얻은 수익 G를 저장\n",
    "    2. 모든 단계에서 행동은 가능한 행동들 중에서 무작위로 선택\n",
    "    3. 그렇게 지정된 횟수 n번만큼(많으면 많을수록 좋음) 에피소드가 끝나면 수익 G들의 평균을 각 상태마다 계산해서 각 상태의 상태가치로 저장\n",
    "    \n",
    "    \n",
    "- 방법\n",
    "    1. First-visit MC : 첫 번째로 도착한 상태의 보상만을 참고하는 방법, 이후 중복된 상태의 보상은 계산에 포함하지 않음\n",
    "    2. Every-visit MC : 거쳐간 모든 상태의 보상을 수익에 참고하는 방법\n",
    "    \n",
    "\n",
    "- 전제조건\n",
    "    1. 모든 상태에서 시작할 수 있어야 한다.\n",
    "    2. 에피소드는 반드시 끝이 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에피소드 생성\n",
    "\n",
    "\n",
    "* i, j : 에피소드 출발 상태 좌표\n",
    "\n",
    "\n",
    "* G : 에피소드에서 얻은 수익\n",
    "\n",
    "\n",
    "* episode : 에피소드를 진행하면서 방문 정보를 저장한 리스트\n",
    "\n",
    "\n",
    "`[방문한 상태, 선택한 행동, 얻은 보상]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, *args, **kwargs):\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 에피소드를 저장할 리스트\n",
    "    episode = []\n",
    "    \n",
    "    # 이전에 방문여부 체크\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    \n",
    "    # 에이전트가 모든 상태에서 출발할 수 있게 출발지점을 무작위로 설정\n",
    "#     i = np.random.randint(0,env.reward.shape[0])\n",
    "#     j = np.random.randint(0,env.reward.shape[1])\n",
    "    i, j = 0, 0\n",
    "    agent.set_pos([i,j])    # 에이전트의 위치 저장\n",
    "    \n",
    "    #에피소드의 수익을 초기화\n",
    "    G = 0\n",
    "    \n",
    "    #감쇄율의 지수\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    \n",
    "    # 에피소드 생성\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()   # 에이전트의 위치 불러오기          \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)  # 선택된 에이전트의 행동결과 반환 (이동한 좌표, 보상 값, 진행가능 여부)\n",
    "        \n",
    "        # 방문 이력 저장 (상태, 행동, 보상)\n",
    "        #[position, action, reward, dummy_G_value = 0]\n",
    "        episode.append([pos, action, reward, 0])\n",
    "\n",
    "        # 에피소드가 종료했다면 루프에서 탈출\n",
    "        if done == True:                \n",
    "            break\n",
    "            \n",
    "    # episode 순간마다 G값 구하기\n",
    "    for ep_i in range(len(episode)-1, -1, -1):\n",
    "        G = G*gamma + episode[ep_i][2] # + reward\n",
    "        episode[ep_i][3] = G\n",
    "            \n",
    "    return i, j, G, episode   # 에피소드 출발 상태 좌표, 수익, 방문 정보를 저장한 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===episode 1===\n",
      "[0, 1]  a: up reward: -3\n",
      "total reward: -3\n",
      "G (감가율 적용한 가치):  -3.0\n",
      "\n",
      " ===episode 2===\n",
      "[1, 1]  a: down reward: -1\n",
      "[2 1]  a: up reward: -1\n",
      "[1 1]  a: left reward: -1\n",
      "[1 0]  a: down reward: -1\n",
      "[2 0]  a: up reward: -1\n",
      "[1 0]  a: up reward: -1\n",
      "[0 0]  a: up reward: -3\n",
      "total reward: -9\n",
      "G (감가율 적용한 가치):  -6.2799130000000005\n",
      "\n",
      " ===episode 3===\n",
      "[2, 1]  a: down reward: -3\n",
      "total reward: -3\n",
      "G (감가율 적용한 가치):  -3.0\n",
      "\n",
      " ===episode 4===\n",
      "[2, 0]  a: right reward: -1\n",
      "[2 1]  a: right reward: 1\n",
      "[2 2]  a: right reward: 1\n",
      "total reward: 1\n",
      "G (감가율 적용한 가치):  0.71\n",
      "\n",
      " ===episode 5===\n",
      "[1, 0]  a: right reward: -1\n",
      "[1 1]  a: up reward: -1\n",
      "[0 1]  a: left reward: -1\n",
      "[0 0]  a: up reward: -3\n",
      "total reward: -6\n",
      "G (감가율 적용한 가치):  -4.897\n",
      "\n",
      " ===episode 6===\n",
      "[1, 2]  a: left reward: -1\n",
      "[1 1]  a: left reward: -1\n",
      "[1 0]  a: up reward: -1\n",
      "[0 0]  a: down reward: -1\n",
      "[1 0]  a: left reward: -3\n",
      "total reward: -7\n",
      "G (감가율 적용한 가치):  -5.4073\n",
      "\n",
      " ===episode 7===\n",
      "[0, 1]  a: left reward: -1\n",
      "[0 0]  a: right reward: -1\n",
      "[0 1]  a: left reward: -1\n",
      "[0 0]  a: left reward: -3\n",
      "total reward: -6\n",
      "G (감가율 적용한 가치):  -4.897\n",
      "\n",
      " ===episode 8===\n",
      "[2, 0]  a: right reward: -1\n",
      "[2 1]  a: right reward: 1\n",
      "[2 2]  a: right reward: 1\n",
      "total reward: 1\n",
      "G (감가율 적용한 가치):  0.71\n",
      "\n",
      " ===episode 9===\n",
      "[0, 2]  a: up reward: -3\n",
      "total reward: -3\n",
      "G (감가율 적용한 가치):  -3.0\n",
      "\n",
      " ===episode 10===\n",
      "[2, 2]  a: left reward: 1\n",
      "total reward: 1\n",
      "G (감가율 적용한 가치):  1.0\n"
     ]
    }
   ],
   "source": [
    "#에피소드 생성 실험\n",
    "np.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\\n ===episode %d===\" % (i+1))\n",
    "    \n",
    "    _, _, G, episode = generate_episode(env, agent, True)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for where, action, reward, G_s in episode:\n",
    "        print(where,\"\", \"a:\", [\"up\", \"right\", \"down\", \"left\"][action], \"reward:\", reward)\n",
    "        total_reward += reward\n",
    "        \n",
    "    print(\"total reward:\", total_reward)\n",
    "    print(\"G (감가율 적용한 가치): \", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit and Every-Visit MC Prediction\n",
    "\n",
    "#### -- \"프로그래머를 위한 강화학습\" 에는 없는 내용 --\n",
    "\n",
    "First-visit MC는 (에피소드 내) 상태 s의 첫 방문시의 반환값만 고려하는 방법\n",
    "\n",
    "Every-visit MC는 (에피소드 내) 상태 s의 모든 방문의 반환값을 고려하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start first visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:03<00:00, 25726.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.80      |      -3.99      |      -3.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.01      |      -3.89      |      -2.42      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.42      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_start_count(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11302.00      |   11148.00      |   11021.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11077.00      |   11027.00      |   11109.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   11151.00      |   11075.00      |   11090.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_success_pr(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.04      |       0.09      |       0.10      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.09      |       0.20      |       0.33      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.10      |       0.33      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "start every visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:04<00:00, 23708.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.81      |      -4.01      |      -3.45      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.02      |      -3.91      |      -2.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.44      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_start_count(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22405.00      |   22257.00      |   22302.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22296.00      |   22070.00      |   22076.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|   22137.00      |   22175.00      |   22282.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "V_success_pr(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.04      |       0.09      |       0.10      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.09      |       0.21      |       0.33      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|       0.10      |       0.33      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# first-visit MC and every-visit MC prediction\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수 𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 에피소드 출발횟수를 저장하는 테이블\n",
    "v_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 도착지점 도착횟수를 저장하는 테이블\n",
    "v_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해)\n",
    "Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수를 지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit 를 사용할지 every visit를 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "\n",
    "for first_visit in [True, False]:\n",
    "    if first_visit:\n",
    "        print(\"start first visit MC\")\n",
    "    else : \n",
    "        print(\"start every visit MC\")\n",
    "    print()\n",
    "\n",
    "    for epi in tqdm(range(max_episode)):\n",
    "\n",
    "        i,j,G,episode = generate_episode(env, agent)\n",
    "\n",
    "        visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "        v_start[i,j] += 1\n",
    "        \n",
    "        for where, action, reward, G_s in episode:\n",
    "            s_i, s_j = where\n",
    "            \n",
    "            if first_visit and visit[s_i][s_j] == 1:\n",
    "                continue\n",
    "                \n",
    "            visit[s_i][s_j] = 1\n",
    "            Return_s[s_i][s_j].append(G_s)\n",
    "\n",
    "            \n",
    "        ## 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n",
    "        # Return_s[i][j].append(G)\n",
    "        ## 에피소드 발생 횟수 계산\n",
    "        # episode_count = len(Return_s[i][j])\n",
    "        ## 상태별 발생한 수익의 총합 계산\n",
    "        # total_G = np.sum(Return_s[i][j])\n",
    "        ## 상태별 발생한 수익의 평균 계산\n",
    "        # v_table[i,j] = total_G / episode_count\n",
    "        # Return_length[i][j].append(len(episode))\n",
    "\n",
    "        # 도착지점에 도착(reward = 1)했는지 체크    \n",
    "        # episode[-1][2] : 에피소드 마지막 상태의 보상\n",
    "        if episode[-1][2] == 1:\n",
    "            v_success[i,j] += 1\n",
    "\n",
    "\n",
    "    # 에피소드 출발 횟수 저장 \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        \n",
    "        for j in range(env.reward.shape[1]):\n",
    "            \n",
    "            visit_count = len(Return_s[i][j])\n",
    "            total_G = np.sum(Return_s[i][j])\n",
    "            v_table[i,j] = total_G / visit_count\n",
    "\n",
    "    print(\"V(s)\")\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print(\"V_start_count(s)\")\n",
    "    show_v_table(np.round(v_start,2),env)\n",
    "    print(\"V_success_pr(s)\")\n",
    "    show_v_table(np.round(v_success/v_start,2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 각 상태들의 상태가치는 도착지점을 향해 점점 커지고 있음\n",
    "    - 상태전이확률을 몰라도 상태가치가 학습 가능하다는 것도 확인 가능  \n",
    "2. 중복된 상태가 있지만, first-visit MC와 Every-visit MC의 상태가치는 거의 동일\n",
    "3. 에피소드의 시작 상태도 무작위로 결정했지만, 거의 동일한 횟수로 각 상태에서 에피소드가 시작했다는 것을 알 수 있음.\n",
    "4. 각 상태에서 출발했을 떄, 도착지점에 도착할 확률을 보면 상태가치와 같이 도착지점에 가까워질수록 확률이 커지는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental mean\n",
    "\n",
    "- `새로운 평균 <- 이전 평균 + 스텝 사이즈 X (새로운 데이터 - 이전 평균)`\n",
    "\n",
    "    - (새로운 데이터 - 이전 평균)이 양수이면 새로운 평균은 증가 / 음수면 감소\n",
    "\n",
    "\n",
    "- 그래서 코드를 수정했는데 `incremental mean을 적용한 코드와 이전 코드의 상태가치함수는 동일함`\n",
    "\n",
    "\n",
    "- 결국, 증분 평균을 사용하면 메모리 사용량이 줄고, 상태가치함수를 에피소드 단위로 간단하게 계산할 수 있음\n",
    "\n",
    "\n",
    "- 몬테카를로 첫 번째 전제조건: \"모든 상태에서 출발할 수 있어야 한다.\"\n",
    "    - But, 앞에서 정의한 문제처럼 에이전트의 시작이 항상 s0에서 시작해야 하면 출발지를 제외한 다른 상태에서 상태가치함수 계산할 수 없음.\n",
    "    - 첫 에피소드를 제외한 나머지 서브 에피소드는 재생성하여, 하나의 독립 에피소드로 다루면 모든 상태에 대해 상태가치함수를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental mean (증분 평균) 을 이용하는 몬테카를로 Prediction 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start first visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:04<00:00, 23994.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.80      |      -4.01      |      -3.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.00      |      -3.90      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.46      |      -2.46      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "start every visit MC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100000/100000 [00:04<00:00, 24056.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.80      |      -4.01      |      -3.44      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.00      |      -3.90      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.45      |      -2.44      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 추가\n",
    "# 상태를 방문한 횟수를 저장하는 테이블\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 삭제\n",
    "# # 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해) : \n",
    "# Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수와 에피소드 최대 길이지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit을 사용할지 every visit을 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "\n",
    "for first_visit in [True, False]:\n",
    "    if first_visit:\n",
    "        print(\"start first visit MC\")\n",
    "    else : \n",
    "        print(\"start every visit MC\")\n",
    "    print()\n",
    "\n",
    "    for epi in tqdm(range(max_episode)):\n",
    "\n",
    "        i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "\n",
    "        visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "        v_start[i,j] += 1\n",
    "        \n",
    "        for where, action, reward, G_s in episode:\n",
    "            s_i, s_j = where\n",
    "            if first_visit and visit[s_i][s_j] == 1:\n",
    "                continue\n",
    "            visit[s_i][s_j] = 1\n",
    "\n",
    "            v_visit[s_i,s_j] += 1\n",
    "            v_table[s_i,s_j] += 1 / v_visit[s_i,s_j] * (G_s - v_table[s_i,s_j])\n",
    "\n",
    "    print(\"V(s)\")\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`앞의 에피소드 생성함수 'generate_episode' 함수에서 i, j 값을 0으로 지정해주고 코드 돌리기`\n",
    "\n",
    "\n",
    "* 결과적으로, 에피소드의 수가 점점 줄어들지만, 상태가치가 커지는 형태가 거의 동일\n",
    "    - 효율적인 계산 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
